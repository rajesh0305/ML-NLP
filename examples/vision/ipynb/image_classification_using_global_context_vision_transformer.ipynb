{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Image Classification using Global Context Vision Transformer\n",
    "\n",
    "**Author:** Md Awsafur Rahman<br>\n",
    "**Date created:** 2023/10/30<br>\n",
    "**Last modified:** 2023/10/30<br>\n",
    "**Description:** Implementation and fine-tuning of Global Context Vision Transformer for image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade keras_cv tensorflow\n",
    "!pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras_cv.layers import DropPath\n",
    "from keras import ops\n",
    "from keras import layers\n",
    "\n",
    "import tensorflow as tf  # only for dataloader\n",
    "import tensorflow_datasets as tfds  # for flower dataset\n",
    "\n",
    "from skimage.data import chelsea\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will utilize multi-backend Keras 3.0 to implement the\n",
    "[**GCViT: Global Context Vision Transformer**](https://arxiv.org/abs/2206.09959) paper,\n",
    "presented at ICML 2023 by A Hatamizadeh et al. The, we will fine-tune the model on the\n",
    "Flower dataset for image classification task, leveraging the official ImageNet pre-trained\n",
    "weights. A highlight of this notebook is its compatibility with multiple backends:\n",
    "TensorFlow, PyTorch, and JAX, showcasing the true potential of multi-backend Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Motivation\n",
    "\n",
    "> **Note:** In this section we'll learn about the backstory of GCViT and try to\n",
    "understand why it is proposed.\n",
    "\n",
    "* During recent years, **Transformers** have achieved dominance in **Natural Language\n",
    "Processing (NLP)** tasks and with the **self-attention** mechanism which allows for\n",
    "capturing both long and short-range information.\n",
    "* Following this trend, **Vision Transformer (ViT)** proposed to utilize image patches as\n",
    "tokens in a gigantic architecture similar to encoder of the original Transformer.\n",
    "* Despite the historic dominance of **Convolutional Neural Network (CNN)** in computer\n",
    "vision, **ViT-based** models have shown **SOTA or competitive performance** in various\n",
    "computer vision tasks.\n",
    "<img src=\"https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/vit_gif.gif\"\n",
    "width=600>\n",
    "\n",
    "* However, **quadratic [`O(n^2)`] computational complexity** of self-attention and **lack\n",
    "of multi-scale information** makes it difficult for **ViT** to be considered as\n",
    "general-purpose architecture for Compute Vision tasks like **segmentation and object\n",
    "detection** where it requires **dense prediction at the pixel level**.\n",
    "* Swin Transformer has attempted to address the issues of **ViT** by proposing\n",
    "**multi-resolution/hierarchical** architectures in which the self-attention is computed\n",
    "in **local windows** and cross-window connections such as **window shifting** are used\n",
    "for modeling the interactions across different regions. But the **limited receptive field\n",
    "of local windows** can not capture long-range information, and cross-window-connection\n",
    "schemes such as **window-shifting only cover a small neighborhood** in the vicinity of\n",
    "each window. Also, it lacks **inductive-bias** that encourages certain translation\n",
    "invariance is still preferable for general-purpose visual modeling, particularly for the\n",
    "dense prediction tasks of object detection and semantic segmentation.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/swin_vs_vit.JPG\"\n",
    "width=400>     <img\n",
    "src=\"https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/shifted_window.JPG\"\n",
    "width=400>\n",
    "<img src=\"https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/swin_arch.JPG\"\n",
    "width=800>\n",
    "\n",
    "* To address above limitations, **Global Context (GC) ViT** network is proposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Architecture\n",
    "\n",
    "Let's have a quick **overview** of our key components,\n",
    "1. `Stem/PatchEmbed:` A stem/patchify layer processes images at the network\u2019s beginning.\n",
    "For this network, it creates **patches/tokens** and converts them into **embeddings**.\n",
    "2. `Level:` It is the repetitive building block that extracts features using different\n",
    "blocks.\n",
    "3. `Global Token Gen./FeatureExtraction:` It generates **global tokens/patches** with\n",
    "**Depthwise-CNN**, **SqueezeAndExcitation (Squeeze-Excitation)**, **CNN** and\n",
    "**MaxPooling**. So basically\n",
    "it's a Feature Extractor.\n",
    "4. `Block:` It is the repetitive module that applies attention to the features and\n",
    "projects them to a certain dimension.\n",
    "        1. `Local-MSA:` Local Multi head Self Attention.\n",
    "        2. `Global-MSA:` Global Multi head Self Attention.\n",
    "        3. `MLP:` Linear layer that projects a vector to another dimension.\n",
    "5. `Downsample/ReduceSize:` It is very similar to **Global Token Gen.** module except it\n",
    "uses **CNN** instead of **MaxPooling** to downsample with additional **Layer\n",
    "Normalization** modules.\n",
    "6. `Head:` It is the module responsible for the classification task.\n",
    "    1. `Pooling:` It converts `N x 2D` features to `N x 1D` features.\n",
    "    2. `Classifier:` It processes `N x 1D` features to make a decision about class.\n",
    "\n",
    "I've annotated the architecture figure to make it easier to digest,\n",
    "<img src=\"https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/arch_annot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Unit Blocks\n",
    "\n",
    "> **Note:** This blocks are used to build other modules throughout the paper. Most of the\n",
    "blocks are either borrowed from other work or modified version old work.\n",
    "\n",
    "1. `SqueezeAndExcitation`: **Squeeze-Excitation (SE)** aka **Bottleneck** module acts sd\n",
    "kind of **channel\n",
    "attention**. It consits of **AvgPooling**, **Dense/FullyConnected (FC)/Linear** ,\n",
    "**GELU** and **Sigmoid** module.\n",
    "<img src=\"https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/se_annot.png\"\n",
    "width=400>\n",
    "\n",
    "2. `Fused-MBConv:` This is similar to the one used in **EfficientNetV2**. It uses\n",
    "**Depthwise-Conv**, **GELU**, **SqueezeAndExcitation**, **Conv**, to extract feature with\n",
    "a resiudal\n",
    "connection. Note that, no new module is declared for this one, we simply applied\n",
    "corresponding modules directly.\n",
    "<img src=\"https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/fmb_annot.png\"\n",
    "width=350>\n",
    "\n",
    "3. `ReduceSize`: It is a **CNN** based **downsample** module which abvobe mentioned\n",
    "`Fused-MBConv` module to extract feature, **Strided Conv** to simultaneously reduce\n",
    "spatial dimension and increse channelwise dimention of the features and finally\n",
    "**LayerNormalization** module to normalize features. In the paper/figure this module is\n",
    "referred as **downsample** module. I think it is mention worthy that **SwniTransformer**\n",
    "used `PatchMerging` module instead of `ReduceSize` to reduce the spatial dimention and\n",
    "increase channelwise dimension which uses **fully-connected/dense/linear** module.\n",
    "According to the **GCViT** paper, one of the purposes of using `ReduceSize` is to add\n",
    "inductive bias through **CNN** module.\n",
    "<img src=\"https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/down_annot.png\"\n",
    "width=300>\n",
    "\n",
    "4. `MLP:` This is our very own **Multi Layer Perceptron** module. This a\n",
    "feed-forward/fully-connected/linear module which simply projects input to an arbitary\n",
    "dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SqueezeAndExcitation(layers.Layer):\n",
    "    \"\"\"Squeeze and excitation block.\n",
    "\n",
    "    Args:\n",
    "        output_dim: output features dimension, if `None` use same dim as input.\n",
    "        expansion: expansion ratio.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dim=None, expansion=0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.expansion = expansion\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        inp = input_shape[-1]\n",
    "        self.output_dim = self.output_dim or inp\n",
    "        self.avg_pool = layers.GlobalAvgPool2D(keepdims=True, name=\"avg_pool\")\n",
    "        self.fc = [\n",
    "            layers.Dense(int(inp * self.expansion), use_bias=False, name=\"fc_0\"),\n",
    "            layers.Activation(\"gelu\", name=\"fc_1\"),\n",
    "            layers.Dense(self.output_dim, use_bias=False, name=\"fc_2\"),\n",
    "            layers.Activation(\"sigmoid\", name=\"fc_3\"),\n",
    "        ]\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.avg_pool(inputs)\n",
    "        for layer in self.fc:\n",
    "            x = layer(x)\n",
    "        return x * inputs\n",
    "\n",
    "\n",
    "class ReduceSize(layers.Layer):\n",
    "    \"\"\"Down-sampling block.\n",
    "\n",
    "    Args:\n",
    "        keepdims: if False spatial dim is reduced and channel dim is increased\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keepdims=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.keepdims = keepdims\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        embed_dim = input_shape[-1]\n",
    "        dim_out = embed_dim if self.keepdims else 2 * embed_dim\n",
    "        self.pad1 = layers.ZeroPadding2D(1, name=\"pad1\")\n",
    "        self.pad2 = layers.ZeroPadding2D(1, name=\"pad2\")\n",
    "        self.conv = [\n",
    "            layers.DepthwiseConv2D(\n",
    "                kernel_size=3, strides=1, padding=\"valid\", use_bias=False, name=\"conv_0\"\n",
    "            ),\n",
    "            layers.Activation(\"gelu\", name=\"conv_1\"),\n",
    "            SqueezeAndExcitation(name=\"conv_2\"),\n",
    "            layers.Conv2D(\n",
    "                embed_dim,\n",
    "                kernel_size=1,\n",
    "                strides=1,\n",
    "                padding=\"valid\",\n",
    "                use_bias=False,\n",
    "                name=\"conv_3\",\n",
    "            ),\n",
    "        ]\n",
    "        self.reduction = layers.Conv2D(\n",
    "            dim_out,\n",
    "            kernel_size=3,\n",
    "            strides=2,\n",
    "            padding=\"valid\",\n",
    "            use_bias=False,\n",
    "            name=\"reduction\",\n",
    "        )\n",
    "        self.norm1 = layers.LayerNormalization(\n",
    "            -1, 1e-05, name=\"norm1\"\n",
    "        )  # eps like PyTorch\n",
    "        self.norm2 = layers.LayerNormalization(-1, 1e-05, name=\"norm2\")\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.norm1(inputs)\n",
    "        xr = self.pad1(x)\n",
    "        for layer in self.conv:\n",
    "            xr = layer(xr)\n",
    "        x = x + xr\n",
    "        x = self.pad2(x)\n",
    "        x = self.reduction(x)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(layers.Layer):\n",
    "    \"\"\"Multi-Layer Perceptron (MLP) block.\n",
    "\n",
    "    Args:\n",
    "        hidden_features: hidden features dimension.\n",
    "        out_features: output features dimension.\n",
    "        activation: activation function.\n",
    "        dropout: dropout rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_features=None,\n",
    "        out_features=None,\n",
    "        activation=\"gelu\",\n",
    "        dropout=0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_features = hidden_features\n",
    "        self.out_features = out_features\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.in_features = input_shape[-1]\n",
    "        self.hidden_features = self.hidden_features or self.in_features\n",
    "        self.out_features = self.out_features or self.in_features\n",
    "        self.fc1 = layers.Dense(self.hidden_features, name=\"fc1\")\n",
    "        self.act = layers.Activation(self.activation, name=\"act\")\n",
    "        self.fc2 = layers.Dense(self.out_features, name=\"fc2\")\n",
    "        self.drop1 = layers.Dropout(self.dropout, name=\"drop1\")\n",
    "        self.drop2 = layers.Dropout(self.dropout, name=\"drop2\")\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Stem\n",
    "\n",
    "> **Notes**: In the code, this module is referred to as **PatchEmbed** but on paper, it\n",
    "is referred to as **Stem**.\n",
    "\n",
    "In the model, we have first used `patch_embed` module. Let's try to understand this\n",
    "module. As we can see from the `call` method,\n",
    "1. This module first **pads** input\n",
    "2. Then uses **convolutions** to extract patches with embeddings.\n",
    "3. Finally, uses `ReduceSize` module to first extract features with **convolution** but\n",
    "neither reduces spatial dimension nor increases spatial dimension.\n",
    "4. One important point to notice, unlike **ViT** or **SwinTransformer**, **GCViT**\n",
    "creates **overlapping patches**. We can notice that from the code,\n",
    "`Conv2D(self.embed_dim, kernel_size=3, strides=2, name='proj')`. If we wanted\n",
    "**non-overlapping** patches then we would've used the same `kernel_size` and `stride`.\n",
    "5. This module reduces the spatial dimension of input by `4x`.\n",
    "> Summary: image \u2192 padding \u2192 convolution \u2192\n",
    "(feature_extract + downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PatchEmbed(layers.Layer):\n",
    "    \"\"\"Patch embedding block.\n",
    "\n",
    "    Args:\n",
    "        embed_dim: feature size dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.pad = layers.ZeroPadding2D(1, name=\"pad\")\n",
    "        self.proj = layers.Conv2D(self.embed_dim, 3, 2, name=\"proj\")\n",
    "        self.conv_down = ReduceSize(keepdims=True, name=\"conv_down\")\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.pad(inputs)\n",
    "        x = self.proj(x)\n",
    "        x = self.conv_down(x)\n",
    "        return x\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Global Token Gen.\n",
    "\n",
    "> **Notes:** It is one of the two **CNN** modules that is used to imppose inductive bias.\n",
    "\n",
    "As we can see from above cell, in the `level` we have first used `to_q_global/Global\n",
    "Token Gen./FeatureExtraction`. Let's try to understand how it works,\n",
    "\n",
    "* This module is series of `FeatureExtract` module, according to paper we need to\n",
    "repeat this module `K` times, where `K = log2(H/h)`, `H = feature_map_height`,\n",
    "`W = feature_map_width`.\n",
    "* `FeatureExtraction:` This layer is very similar to `ReduceSize` module except it uses\n",
    "**MaxPooling** module to reduce the dimension, it doesn't increse feature dimension\n",
    "(channelsie) and it doesn't uses **LayerNormalizaton**. This module is used to in\n",
    "`Generate Token Gen.` module repeatedly to generte **global tokens** for\n",
    "**global-context-attention**.\n",
    "* One important point to notice from the figure is that, **global tokens** is shared\n",
    "across the whole image which means we use only **one global window** for **all local\n",
    "tokens** in a image. This makes the computation very efficient.\n",
    "* For input feature map with shape `(B, H, W, C)`, we'll get output shape `(B, h, w, C)`.\n",
    "If we copy these global tokens for total `M` local windows in an image where,\n",
    "`M = (H x W)/(h x w) = num_window`, then output shape: `(B * M, h, w, C)`.\"\n",
    "\n",
    "> Summary: This module is used to `resize` the image to fit window.\n",
    "\n",
    "<img\n",
    "src=\"https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/global_token_annot.png\"\n",
    "width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FeatureExtraction(layers.Layer):\n",
    "    \"\"\"Feature extraction block.\n",
    "\n",
    "    Args:\n",
    "        keepdims: bool argument for maintaining the resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keepdims=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.keepdims = keepdims\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        embed_dim = input_shape[-1]\n",
    "        self.pad1 = layers.ZeroPadding2D(1, name=\"pad1\")\n",
    "        self.pad2 = layers.ZeroPadding2D(1, name=\"pad2\")\n",
    "        self.conv = [\n",
    "            layers.DepthwiseConv2D(3, 1, use_bias=False, name=\"conv_0\"),\n",
    "            layers.Activation(\"gelu\", name=\"conv_1\"),\n",
    "            SqueezeAndExcitation(name=\"conv_2\"),\n",
    "            layers.Conv2D(embed_dim, 1, 1, use_bias=False, name=\"conv_3\"),\n",
    "        ]\n",
    "        if not self.keepdims:\n",
    "            self.pool = layers.MaxPool2D(3, 2, name=\"pool\")\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = inputs\n",
    "        xr = self.pad1(x)\n",
    "        for layer in self.conv:\n",
    "            xr = layer(xr)\n",
    "        x = x + xr\n",
    "        if not self.keepdims:\n",
    "            x = self.pool(self.pad2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GlobalQueryGenerator(layers.Layer):\n",
    "    \"\"\"Global query generator.\n",
    "\n",
    "    Args:\n",
    "        keepdims: to keep the dimension of FeatureExtraction layer.\n",
    "        For instance, repeating log(56/7) = 3 blocks, with input\n",
    "        window dimension 56 and output window dimension 7 at down-sampling\n",
    "        ratio 2. Please check Fig.5 of GC ViT paper for details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keepdims=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.keepdims = keepdims\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.to_q_global = [\n",
    "            FeatureExtraction(keepdims, name=f\"to_q_global_{i}\")\n",
    "            for i, keepdims in enumerate(self.keepdims)\n",
    "        ]\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = inputs\n",
    "        for layer in self.to_q_global:\n",
    "            x = layer(x)\n",
    "        return x\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Attention\n",
    "\n",
    "> **Notes:** This is the core contribution of the paper.\n",
    "\n",
    "As we can see from the `call` method,\n",
    "1. `WindowAttention` module applies both **local** and **global** window attention\n",
    "depending on `global_query` parameter.\n",
    "\n",
    "2. First it converts input features into `query, key, value` for local attention and\n",
    "`key, value` for global attention. For global attention, it takes global query from\n",
    "`Global Token Gen.`. One thing to notice from the code is that we divide the **features\n",
    "or embed_dim** among all the **heads of Transformer** to reduce the computation.\n",
    "`qkv = tf.reshape(qkv, [B_, N, self.qkv_size, self.num_heads, C // self.num_heads])`\n",
    "3. Before sending query, key and value for attention, **global token** goes through an\n",
    "important process. Same global tokens or one global window gets copied for all the local\n",
    "windows to increase efficiency.\n",
    "`q_global = tf.repeat(q_global, repeats=B_//B, axis=0)`, here `B_//B` means `num_windows`\n",
    "in a image.\n",
    "4. Then simply applies `local-window-self-attention` or `global-window-attention`\n",
    "depending on `global_query` parameter. One thing to notice from the code is that we are\n",
    "adding **relative-positional-embedding** with the **attention mask** instead of the\n",
    "**patch embedding**.\n",
    "`attn = attn + relative_position_bias[tf.newaxis,]`\n",
    "<img src=\"https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/lvg_msa.PNG\"\n",
    "width=800>\n",
    "5. Now, let's think for a bit and try to understand what is happening here. Let's focus\n",
    "on the figure below. We can see from the left, that in the **local-attention** the\n",
    "**query is local** and it's **limited to the local window** (red square border) hence we\n",
    "don't have access to long-range information. But on the right that due to **global\n",
    "query** we're now **not limited to local-windows** (blue square border) and we have\n",
    "access to long-range information.\n",
    "<img src=\"https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/lvg_arch.PNG\"\n",
    "width=800>\n",
    "6. In **ViT** we compare (attention) image-tokens with image-tokens, in\n",
    "**SwinTransformer** we compare window-tokens with window-tokens but in **GCViT** we\n",
    "compare image-tokens with window-tokens. But now you may ask, how can compare(attention)\n",
    "image-tokens with window-tokens even after image-tokens have larger dimensions than\n",
    "window-tokens? (from above figure image-tokens have shape `(1, 8, 8, 3)` and\n",
    "window-tokens have shape `(1, 4, 4, 3)`). Yes, you are right we can't directly compare\n",
    "them hence we resize image-tokens to fit window-tokens with `Global Token\n",
    "Gen./FeatureExtraction` **CNN** module. The following table should give you a clear\n",
    "comparison,\n",
    "\n",
    "| Model            | Query Tokens    | Key-Value Tokens  | Attention Type            | Attention Coverage |\n",
    "|------------------|-----------------|-------------------|---------------------------|--------------------|\n",
    "| ViT              | image           | image             | self-attention            | global             |\n",
    "| SwinTransformer  | window          | window            | self-attention            | local              |\n",
    "| **GCViT**        | **resized-image** | **window**     | **image-window attention** | **global**        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class WindowAttention(layers.Layer):\n",
    "    \"\"\"Local window attention.\n",
    "\n",
    "    This implementation was proposed by\n",
    "    [Liu et al., 2021](https://arxiv.org/abs/2103.14030) in SwinTransformer.\n",
    "\n",
    "    Args:\n",
    "        window_size: window size.\n",
    "        num_heads: number of attention head.\n",
    "        global_query: if the input contains global_query\n",
    "        qkv_bias: bool argument for query, key, value learnable bias.\n",
    "        qk_scale: bool argument to scaling query, key.\n",
    "        attention_dropout: attention dropout rate.\n",
    "        projection_dropout: output dropout rate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        window_size,\n",
    "        num_heads,\n",
    "        global_query,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        attention_dropout=0.0,\n",
    "        projection_dropout=0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        window_size = (window_size, window_size)\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.global_query = global_query\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.qk_scale = qk_scale\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.projection_dropout = projection_dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        embed_dim = input_shape[0][-1]\n",
    "        head_dim = embed_dim // self.num_heads\n",
    "        self.scale = self.qk_scale or head_dim**-0.5\n",
    "        self.qkv_size = 3 - int(self.global_query)\n",
    "        self.qkv = layers.Dense(\n",
    "            embed_dim * self.qkv_size, use_bias=self.qkv_bias, name=\"qkv\"\n",
    "        )\n",
    "        self.relative_position_bias_table = self.add_weight(\n",
    "            name=\"relative_position_bias_table\",\n",
    "            shape=[\n",
    "                (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1),\n",
    "                self.num_heads,\n",
    "            ],\n",
    "            initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "            trainable=True,\n",
    "            dtype=self.dtype,\n",
    "        )\n",
    "        self.attn_drop = layers.Dropout(self.attention_dropout, name=\"attn_drop\")\n",
    "        self.proj = layers.Dense(embed_dim, name=\"proj\")\n",
    "        self.proj_drop = layers.Dropout(self.projection_dropout, name=\"proj_drop\")\n",
    "        self.softmax = layers.Activation(\"softmax\", name=\"softmax\")\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def get_relative_position_index(self):\n",
    "        coords_h = ops.arange(self.window_size[0])\n",
    "        coords_w = ops.arange(self.window_size[1])\n",
    "        coords = ops.stack(ops.meshgrid(coords_h, coords_w, indexing=\"ij\"), axis=0)\n",
    "        coords_flatten = ops.reshape(coords, [2, -1])\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = ops.transpose(relative_coords, axes=[1, 2, 0])\n",
    "        relative_coords_xx = relative_coords[:, :, 0] + self.window_size[0] - 1\n",
    "        relative_coords_yy = relative_coords[:, :, 1] + self.window_size[1] - 1\n",
    "        relative_coords_xx = relative_coords_xx * (2 * self.window_size[1] - 1)\n",
    "        relative_position_index = relative_coords_xx + relative_coords_yy\n",
    "        return relative_position_index\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.global_query:\n",
    "            inputs, q_global = inputs\n",
    "            B = ops.shape(q_global)[0]  # B, N, C\n",
    "        else:\n",
    "            inputs = inputs[0]\n",
    "        B_, N, C = ops.shape(inputs)  # B*num_window, num_tokens, channels\n",
    "        qkv = self.qkv(inputs)\n",
    "        qkv = ops.reshape(\n",
    "            qkv, [B_, N, self.qkv_size, self.num_heads, C // self.num_heads]\n",
    "        )\n",
    "        qkv = ops.transpose(qkv, [2, 0, 3, 1, 4])\n",
    "        if self.global_query:\n",
    "            k, v = ops.split(\n",
    "                qkv, indices_or_sections=2, axis=0\n",
    "            )  # for unknown shame num=None will throw error\n",
    "            q_global = ops.repeat(\n",
    "                q_global, repeats=B_ // B, axis=0\n",
    "            )  # num_windows = B_//B => q_global same for all windows in a img\n",
    "            q = ops.reshape(q_global, [B_, N, self.num_heads, C // self.num_heads])\n",
    "            q = ops.transpose(q, axes=[0, 2, 1, 3])\n",
    "        else:\n",
    "            q, k, v = ops.split(qkv, indices_or_sections=3, axis=0)\n",
    "            q = ops.squeeze(q, axis=0)\n",
    "\n",
    "        k = ops.squeeze(k, axis=0)\n",
    "        v = ops.squeeze(v, axis=0)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = q @ ops.transpose(k, axes=[0, 1, 3, 2])\n",
    "        relative_position_bias = ops.take(\n",
    "            self.relative_position_bias_table,\n",
    "            ops.reshape(self.get_relative_position_index(), [-1]),\n",
    "        )\n",
    "        relative_position_bias = ops.reshape(\n",
    "            relative_position_bias,\n",
    "            [\n",
    "                self.window_size[0] * self.window_size[1],\n",
    "                self.window_size[0] * self.window_size[1],\n",
    "                -1,\n",
    "            ],\n",
    "        )\n",
    "        relative_position_bias = ops.transpose(relative_position_bias, axes=[2, 0, 1])\n",
    "        attn = attn + relative_position_bias[None,]\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = ops.transpose((attn @ v), axes=[0, 2, 1, 3])\n",
    "        x = ops.reshape(x, [B_, N, C])\n",
    "        x = self.proj_drop(self.proj(x))\n",
    "        return x\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Block\n",
    "\n",
    "> **Notes:** This module doesn't have any Convolutional module.\n",
    "\n",
    "In the `level` second module that we have used is `block`. Let's try to understand how it\n",
    "works. As we can see from the `call` method,\n",
    "1. `Block` module takes either only feature_maps for local attention or additional global\n",
    "query for global attention.\n",
    "2. Before sending feature maps for attention, this module converts **batch feature maps**\n",
    "to **batch windows** as we'll be applying **Window Attention**.\n",
    "3. Then we send batch **batch windows** for attention.\n",
    "4. After attention has been applied we revert **batch windows** to **batch feature maps**.\n",
    "5. Before sending the attention to applied features for output, this module applies\n",
    "**Stochastic Depth** regularization in the residual connection. Also, before applying\n",
    "**Stochastic Depth** it rescales the input with trainable parameters. Note that, this\n",
    "**Stochastic Depth** block hasn't been shown in the figure of the paper.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/block2.JPG\"\n",
    "width=400>\n",
    "\n",
    "\n",
    "### Window\n",
    "In the `block` module, we have created **windows** before and after applying attention.\n",
    "Let's try to understand how we're creating windows,\n",
    "* Following module converts feature maps `(B, H, W, C)` to stacked windows\n",
    "`(B x H/h x W/w, h, w, C)` \u2192 `(num_windows_batch, window_size, window_size, channel)`\n",
    "* This module uses `reshape` & `transpose` to create these windows out of image instead\n",
    "of iterating over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Block(layers.Layer):\n",
    "    \"\"\"GCViT block.\n",
    "\n",
    "    Args:\n",
    "        window_size: window size.\n",
    "        num_heads: number of attention head.\n",
    "        global_query: apply global window attention\n",
    "        mlp_ratio: MLP ratio.\n",
    "        qkv_bias: bool argument for query, key, value learnable bias.\n",
    "        qk_scale: bool argument to scaling query, key.\n",
    "        drop: dropout rate.\n",
    "        attention_dropout: attention dropout rate.\n",
    "        path_drop: drop path rate.\n",
    "        activation: activation function.\n",
    "        layer_scale: layer scaling coefficient.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        window_size,\n",
    "        num_heads,\n",
    "        global_query,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        dropout=0.0,\n",
    "        attention_dropout=0.0,\n",
    "        path_drop=0.0,\n",
    "        activation=\"gelu\",\n",
    "        layer_scale=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.global_query = global_query\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.qk_scale = qk_scale\n",
    "        self.dropout = dropout\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.path_drop = path_drop\n",
    "        self.activation = activation\n",
    "        self.layer_scale = layer_scale\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        B, H, W, C = input_shape[0]\n",
    "        self.norm1 = layers.LayerNormalization(-1, 1e-05, name=\"norm1\")\n",
    "        self.attn = WindowAttention(\n",
    "            window_size=self.window_size,\n",
    "            num_heads=self.num_heads,\n",
    "            global_query=self.global_query,\n",
    "            qkv_bias=self.qkv_bias,\n",
    "            qk_scale=self.qk_scale,\n",
    "            attention_dropout=self.attention_dropout,\n",
    "            projection_dropout=self.dropout,\n",
    "            name=\"attn\",\n",
    "        )\n",
    "        self.drop_path1 = DropPath(self.path_drop)\n",
    "        self.drop_path2 = DropPath(self.path_drop)\n",
    "        self.norm2 = layers.LayerNormalization(-1, 1e-05, name=\"norm2\")\n",
    "        self.mlp = MLP(\n",
    "            hidden_features=int(C * self.mlp_ratio),\n",
    "            dropout=self.dropout,\n",
    "            activation=self.activation,\n",
    "            name=\"mlp\",\n",
    "        )\n",
    "        if self.layer_scale is not None:\n",
    "            self.gamma1 = self.add_weight(\n",
    "                name=\"gamma1\",\n",
    "                shape=[C],\n",
    "                initializer=keras.initializers.Constant(self.layer_scale),\n",
    "                trainable=True,\n",
    "                dtype=self.dtype,\n",
    "            )\n",
    "            self.gamma2 = self.add_weight(\n",
    "                name=\"gamma2\",\n",
    "                shape=[C],\n",
    "                initializer=keras.initializers.Constant(self.layer_scale),\n",
    "                trainable=True,\n",
    "                dtype=self.dtype,\n",
    "            )\n",
    "        else:\n",
    "            self.gamma1 = 1.0\n",
    "            self.gamma2 = 1.0\n",
    "        self.num_windows = int(H // self.window_size) * int(W // self.window_size)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.global_query:\n",
    "            inputs, q_global = inputs\n",
    "        else:\n",
    "            inputs = inputs[0]\n",
    "        B, H, W, C = ops.shape(inputs)\n",
    "        x = self.norm1(inputs)\n",
    "        # create windows and concat them in batch axis\n",
    "        x = self.window_partition(x, self.window_size)  # (B_, win_h, win_w, C)\n",
    "        # flatten patch\n",
    "        x = ops.reshape(x, [-1, self.window_size * self.window_size, C])\n",
    "        # attention\n",
    "        if self.global_query:\n",
    "            x = self.attn([x, q_global])\n",
    "        else:\n",
    "            x = self.attn([x])\n",
    "        # reverse window partition\n",
    "        x = self.window_reverse(x, self.window_size, H, W, C)\n",
    "        # FFN\n",
    "        x = inputs + self.drop_path1(x * self.gamma1)\n",
    "        x = x + self.drop_path2(self.gamma2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "    def window_partition(self, x, window_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, H, W, C)\n",
    "            window_size: window size\n",
    "        Returns:\n",
    "            local window features (num_windows*B, window_size, window_size, C)\n",
    "        \"\"\"\n",
    "        B, H, W, C = ops.shape(x)\n",
    "        x = ops.reshape(\n",
    "            x,\n",
    "            [\n",
    "                -1,\n",
    "                H // window_size,\n",
    "                window_size,\n",
    "                W // window_size,\n",
    "                window_size,\n",
    "                C,\n",
    "            ],\n",
    "        )\n",
    "        x = ops.transpose(x, axes=[0, 1, 3, 2, 4, 5])\n",
    "        windows = ops.reshape(x, [-1, window_size, window_size, C])\n",
    "        return windows\n",
    "\n",
    "    def window_reverse(self, windows, window_size, H, W, C):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            windows: local window features (num_windows*B, window_size, window_size, C)\n",
    "            window_size: Window size\n",
    "            H: Height of image\n",
    "            W: Width of image\n",
    "            C: Channel of image\n",
    "        Returns:\n",
    "            x: (B, H, W, C)\n",
    "        \"\"\"\n",
    "        x = ops.reshape(\n",
    "            windows,\n",
    "            [\n",
    "                -1,\n",
    "                H // window_size,\n",
    "                W // window_size,\n",
    "                window_size,\n",
    "                window_size,\n",
    "                C,\n",
    "            ],\n",
    "        )\n",
    "        x = ops.transpose(x, axes=[0, 1, 3, 2, 4, 5])\n",
    "        x = ops.reshape(x, [-1, H, W, C])\n",
    "        return x\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Level\n",
    "\n",
    "> **Note:** This module has both Transformer and CNN modules.\n",
    "\n",
    "In the model, the second module that we have used is `level`. Let's try to understand\n",
    "this module. As we can see from the `call` method,\n",
    "1. First it creates **global_token** with a series of `FeatureExtraction` modules. As\n",
    "we'll see\n",
    "later that `FeatureExtraction` is nothing but a simple **CNN** based module.\n",
    "2. Then it uses series of`Block` modules to apply **local or global window attention**\n",
    "depending on depth level.\n",
    "3. Finally, it uses `ReduceSize` to reduce the dimension of **contextualized features**.\n",
    "\n",
    "> Summary: feature_map \u2192 global_token \u2192 local/global window\n",
    "attention \u2192 dowsample\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/awsaf49/gcvit-tf/main/image/level.png\"\n",
    "width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Level(layers.Layer):\n",
    "    \"\"\"GCViT level.\n",
    "\n",
    "    Args:\n",
    "        depth: number of layers in each stage.\n",
    "        num_heads: number of heads in each stage.\n",
    "        window_size: window size in each stage.\n",
    "        keepdims: dims to keep in FeatureExtraction.\n",
    "        downsample: bool argument for down-sampling.\n",
    "        mlp_ratio: MLP ratio.\n",
    "        qkv_bias: bool argument for query, key, value learnable bias.\n",
    "        qk_scale: bool argument to scaling query, key.\n",
    "        drop: dropout rate.\n",
    "        attention_dropout: attention dropout rate.\n",
    "        path_drop: drop path rate.\n",
    "        layer_scale: layer scaling coefficient.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        keepdims,\n",
    "        downsample=True,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        dropout=0.0,\n",
    "        attention_dropout=0.0,\n",
    "        path_drop=0.0,\n",
    "        layer_scale=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.depth = depth\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.keepdims = keepdims\n",
    "        self.downsample = downsample\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.qk_scale = qk_scale\n",
    "        self.dropout = dropout\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.path_drop = path_drop\n",
    "        self.layer_scale = layer_scale\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        path_drop = (\n",
    "            [self.path_drop] * self.depth\n",
    "            if not isinstance(self.path_drop, list)\n",
    "            else self.path_drop\n",
    "        )\n",
    "        self.blocks = [\n",
    "            Block(\n",
    "                window_size=self.window_size,\n",
    "                num_heads=self.num_heads,\n",
    "                global_query=bool(i % 2),\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                qkv_bias=self.qkv_bias,\n",
    "                qk_scale=self.qk_scale,\n",
    "                dropout=self.dropout,\n",
    "                attention_dropout=self.attention_dropout,\n",
    "                path_drop=path_drop[i],\n",
    "                layer_scale=self.layer_scale,\n",
    "                name=f\"blocks_{i}\",\n",
    "            )\n",
    "            for i in range(self.depth)\n",
    "        ]\n",
    "        self.down = ReduceSize(keepdims=False, name=\"downsample\")\n",
    "        self.q_global_gen = GlobalQueryGenerator(self.keepdims, name=\"q_global_gen\")\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = inputs\n",
    "        q_global = self.q_global_gen(x)  # shape: (B, win_size, win_size, C)\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i % 2:\n",
    "                x = blk([x, q_global])  # shape: (B, H, W, C)\n",
    "            else:\n",
    "                x = blk([x])  # shape: (B, H, W, C)\n",
    "        if self.downsample:\n",
    "            x = self.down(x)  # shape: (B, H//2, W//2, 2*C)\n",
    "        return x\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Model\n",
    "\n",
    "Let's directly jump to the model. As we can see from the `call` method,\n",
    "1. It creates patch embeddings from an image. This layer doesn't flattens these\n",
    "embeddings which means output of this module will be\n",
    "`(batch, height/window_size, width/window_size, embed_dim)` instead of\n",
    "`(batch, height x width/window_size^2, embed_dim)`.\n",
    "2. Then it applies `Dropout` module which randomly sets input units to 0.\n",
    "3. It passes these embeddings to series of `Level` modules which we are calling `level`\n",
    "where,\n",
    "    1. Global token is generated\n",
    "    1. Both local & global attention is applied\n",
    "    1. Finally downsample is applied.\n",
    "4. So, output after `n` number of **levels**, shape: `(batch, width/window_size x 2^{n-1},\n",
    "width/window_size x 2^{n-1}, embed_dim x 2^{n-1})`. In the last layer,\n",
    "paper doesn't use **downsample** and increase **channels**.\n",
    "5. Output of above layer is normalized using `LayerNormalization` module.\n",
    "6. In the head, 2D features are converted to 1D features with `Pooling` module. Output\n",
    "shape after this module is `(batch, embed_dim x 2^{n-1})`\n",
    "7. Finally, pooled features are sent to `Dense/Linear` module for classification.\n",
    "\n",
    "> Sumamry: image \u2192 (patchs + embedding) \u2192 dropout\n",
    "\u2192 (attention + feature extraction) \u2192 normalizaion \u2192\n",
    "pooling \u2192 classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class GCViT(keras.Model):\n",
    "    \"\"\"GCViT model.\n",
    "\n",
    "    Args:\n",
    "        window_size: window size in each stage.\n",
    "        embed_dim: feature size dimension.\n",
    "        depths: number of layers in each stage.\n",
    "        num_heads: number of heads in each stage.\n",
    "        drop_rate: dropout rate.\n",
    "        mlp_ratio: MLP ratio.\n",
    "        qkv_bias: bool argument for query, key, value learnable bias.\n",
    "        qk_scale: bool argument to scaling query, key.\n",
    "        attention_dropout: attention dropout rate.\n",
    "        path_drop: drop path rate.\n",
    "        layer_scale: layer scaling coefficient.\n",
    "        num_classes: number of classes.\n",
    "        head_activation: activation function for head.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        window_size,\n",
    "        embed_dim,\n",
    "        depths,\n",
    "        num_heads,\n",
    "        drop_rate=0.0,\n",
    "        mlp_ratio=3.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        attention_dropout=0.0,\n",
    "        path_drop=0.1,\n",
    "        layer_scale=None,\n",
    "        num_classes=1000,\n",
    "        head_activation=\"softmax\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.window_size = window_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.depths = depths\n",
    "        self.num_heads = num_heads\n",
    "        self.drop_rate = drop_rate\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.qk_scale = qk_scale\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.path_drop = path_drop\n",
    "        self.layer_scale = layer_scale\n",
    "        self.num_classes = num_classes\n",
    "        self.head_activation = head_activation\n",
    "\n",
    "        self.patch_embed = PatchEmbed(embed_dim=embed_dim, name=\"patch_embed\")\n",
    "        self.pos_drop = layers.Dropout(drop_rate, name=\"pos_drop\")\n",
    "        path_drops = np.linspace(0.0, path_drop, sum(depths))\n",
    "        keepdims = [(0, 0, 0), (0, 0), (1,), (1,)]\n",
    "        self.levels = []\n",
    "        for i in range(len(depths)):\n",
    "            path_drop = path_drops[sum(depths[:i]) : sum(depths[: i + 1])].tolist()\n",
    "            level = Level(\n",
    "                depth=depths[i],\n",
    "                num_heads=num_heads[i],\n",
    "                window_size=window_size[i],\n",
    "                keepdims=keepdims[i],\n",
    "                downsample=(i < len(depths) - 1),\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                dropout=drop_rate,\n",
    "                attention_dropout=attention_dropout,\n",
    "                path_drop=path_drop,\n",
    "                layer_scale=layer_scale,\n",
    "                name=f\"levels_{i}\",\n",
    "            )\n",
    "            self.levels.append(level)\n",
    "        self.norm = layers.LayerNormalization(axis=-1, epsilon=1e-05, name=\"norm\")\n",
    "        self.pool = layers.GlobalAvgPool2D(name=\"pool\")\n",
    "        self.head = layers.Dense(num_classes, name=\"head\", activation=head_activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x = self.patch_embed(inputs)  # shape: (B, H, W, C)\n",
    "        x = self.pos_drop(x)\n",
    "        for level in self.levels:\n",
    "            x = level(x)  # shape: (B, H_, W_, C_)\n",
    "        x = self.norm(x)\n",
    "        x = self.pool(x)  # shape: (B, C__)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def build_graph(self, input_shape=(224, 224, 3)):\n",
    "        \"\"\"\n",
    "        ref: https://www.kaggle.com/code/ipythonx/tf-hybrid-efficientnet-swin-transformer-gradcam\n",
    "        \"\"\"\n",
    "        x = keras.Input(shape=input_shape)\n",
    "        return keras.Model(inputs=[x], outputs=self.call(x), name=self.name)\n",
    "\n",
    "    def summary(self, input_shape=(224, 224, 3)):\n",
    "        return self.build_graph(input_shape).summary()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Build Model\n",
    "\n",
    "* Let's build a complete model with all the modules that we've explained above. We'll\n",
    "build **GCViT-XXTiny** model with the configuration mentioned in the paper.\n",
    "* Also we'll load the ported official **pre-trained** weights and try for some\n",
    "predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Model Configs\n",
    "config = {\n",
    "    \"window_size\": (7, 7, 14, 7),\n",
    "    \"embed_dim\": 64,\n",
    "    \"depths\": (2, 2, 6, 2),\n",
    "    \"num_heads\": (2, 4, 8, 16),\n",
    "    \"mlp_ratio\": 3.0,\n",
    "    \"path_drop\": 0.2,\n",
    "}\n",
    "ckpt_link = (\n",
    "    \"https://github.com/awsaf49/gcvit-tf/releases/download/v1.1.6/gcvitxxtiny.keras\"\n",
    ")\n",
    "\n",
    "# Build Model\n",
    "model = GCViT(**config)\n",
    "inp = ops.array(np.random.uniform(size=(1, 224, 224, 3)))\n",
    "out = model(inp)\n",
    "\n",
    "# Load Weights\n",
    "ckpt_path = keras.utils.get_file(ckpt_link.split(\"/\")[-1], ckpt_link)\n",
    "model.load_weights(ckpt_path)\n",
    "\n",
    "# Summary\n",
    "model.summary((224, 224, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Sanity check for Pre-Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "img = keras.applications.imagenet_utils.preprocess_input(\n",
    "    chelsea(), mode=\"torch\"\n",
    ")  # Chelsea the cat\n",
    "img = ops.image.resize(img, (224, 224))[None,]  # resize & create batch\n",
    "pred = model(img)\n",
    "pred_dec = keras.applications.imagenet_utils.decode_predictions(pred)[0]\n",
    "\n",
    "print(\"\\n# Image:\")\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(chelsea())\n",
    "plt.show()\n",
    "print()\n",
    "\n",
    "print(\"# Prediction (Top 5):\")\n",
    "for i in range(5):\n",
    "    print(\"{:<12} : {:0.2f}\".format(pred_dec[i][1], pred_dec[i][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Fine-tune **GCViT** Model\n",
    "\n",
    "In the following cells, we will fine-tune **GCViT** model on Flower Dataset which\n",
    "consists `104` classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "# Hyper Params\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "\n",
    "# Dataset\n",
    "CLASSES = [\n",
    "    \"dandelion\",\n",
    "    \"daisy\",\n",
    "    \"tulips\",\n",
    "    \"sunflowers\",\n",
    "    \"roses\",\n",
    "]  # don't change the order\n",
    "\n",
    "# Other constants\n",
    "MEAN = 255 * np.array([0.485, 0.456, 0.406], dtype=\"float32\")  # imagenet mean\n",
    "STD = 255 * np.array([0.229, 0.224, 0.225], dtype=\"float32\")  # imagenet std\n",
    "AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_dataset(dataset: tf.data.Dataset, train: bool, image_size: int = IMAGE_SIZE):\n",
    "    def preprocess(image, label):\n",
    "        # for training, do augmentation\n",
    "        if train:\n",
    "            if tf.random.uniform(shape=[]) > 0.5:\n",
    "                image = tf.image.flip_left_right(image)\n",
    "        image = tf.image.resize(image, size=image_size, method=\"bicubic\")\n",
    "        image = (image - MEAN) / STD  # normalization\n",
    "        return image, label\n",
    "\n",
    "    if train:\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 10)\n",
    "\n",
    "    return dataset.map(preprocess, AUTO).batch(BATCH_SIZE).prefetch(AUTO)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Flower Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:90%]\", \"train[90%:]\"],\n",
    "    as_supervised=True,\n",
    "    try_gcs=False,  # gcs_path is necessary for tpu,\n",
    ")\n",
    "\n",
    "train_dataset = make_dataset(train_dataset, True)\n",
    "val_dataset = make_dataset(val_dataset, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Re-Build Model for Flower Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Re-Build Model\n",
    "model = GCViT(**config, num_classes=104)\n",
    "inp = ops.array(np.random.uniform(size=(1, 224, 224, 3)))\n",
    "out = model(inp)\n",
    "\n",
    "# Load Weights\n",
    "ckpt_path = keras.utils.get_file(ckpt_link.split(\"/\")[-1], ckpt_link)\n",
    "model.load_weights(ckpt_path, skip_mismatch=True)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset, validation_data=val_dataset, epochs=EPOCHS, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Reference\n",
    "\n",
    "* [gcvit-tf - A Python library for GCViT with TF2.0](https://github.com/awsaf49/gcvit-tf)\n",
    "* [gcvit - Official codebase for GCViT](https://github.com/NVlabs/GCVit)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "image_classification_using_global_context_vision_transformer",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}