{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Electroencephalogram Signal Classification for action identification\n",
    "\n",
    "**Author:** [Suvaditya Mukherjee](https://github.com/suvadityamuk)<br>\n",
    "**Date created:** 2022/11/03<br>\n",
    "**Last modified:** 2022/11/05<br>\n",
    "**Description:** Training a Convolutional model to classify EEG signals produced by exposure to certain stimuli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The following example explores how we can make a Convolution-based Neural Network to\n",
    "perform classification on Electroencephalogram signals captured when subjects were\n",
    "exposed to different stimuli.\n",
    "We train a model from scratch since such signal-classification models are fairly scarce\n",
    "in pre-trained format.\n",
    "The data we use is sourced from the UC Berkeley-Biosense Lab where the data was collected\n",
    "from 15 subjects at the same time.\n",
    "Our process is as follows:\n",
    "\n",
    "- Load the [UC Berkeley-Biosense Synchronized Brainwave Dataset](https://www.kaggle.com/datasets/berkeley-biosense/synchronized-brainwave-dataset)\n",
    "- Visualize random samples from the data\n",
    "- Pre-process, collate and scale the data to finally make a `tf.data.Dataset`\n",
    "- Prepare class weights in order to tackle major imbalances\n",
    "- Create a Conv1D and Dense-based model to perform classification\n",
    "- Define callbacks and hyperparameters\n",
    "- Train the model\n",
    "- Plot metrics from History and perform evaluation\n",
    "\n",
    "This example needs the following external dependencies (Gdown, Scikit-learn, Pandas,\n",
    "Numpy, Matplotlib). You can install it via the following commands.\n",
    "\n",
    "Gdown is an external package used to download large files from Google Drive. To know\n",
    "more, you can refer to its [PyPi page here](https://pypi.org/project/gdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup and Data Downloads\n",
    "\n",
    "First, lets install our dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install gdown -q\n",
    "!pip install sklearn -q\n",
    "!pip install pandas -q\n",
    "!pip install numpy -q\n",
    "!pip install matplotlib -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Next, lets download our dataset.\n",
    "The gdown package makes it easy to download the data from Google Drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!gdown 1V5B7Bt6aJm0UHbR7cRKBEK8jx7lYPVuX\n",
    "!# gdown will download eeg-data.csv onto the local drive for use. Total size of\n",
    "!# eeg-data.csv is 105.7 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing, model_selection\n",
    "import random\n",
    "\n",
    "QUALITY_THRESHOLD = 128\n",
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = BATCH_SIZE * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Read data from `eeg-data.csv`\n",
    "\n",
    "We use the Pandas library to read the `eeg-data.csv` file and display the first 5 rows\n",
    "using the `.head()` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "eeg = pd.read_csv(\"eeg-data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We remove unlabeled samples from our dataset as they do not contribute to the model. We\n",
    "also perform a `.drop()` operation on the columns that are not required for training data\n",
    "preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "unlabeled_eeg = eeg[eeg[\"label\"] == \"unlabeled\"]\n",
    "eeg = eeg.loc[eeg[\"label\"] != \"unlabeled\"]\n",
    "eeg = eeg.loc[eeg[\"label\"] != \"everyone paired\"]\n",
    "\n",
    "eeg.drop(\n",
    "    [\n",
    "        \"indra_time\",\n",
    "        \"Unnamed: 0\",\n",
    "        \"browser_latency\",\n",
    "        \"reading_time\",\n",
    "        \"attention_esense\",\n",
    "        \"meditation_esense\",\n",
    "        \"updatedAt\",\n",
    "        \"createdAt\",\n",
    "    ],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "eeg.reset_index(drop=True, inplace=True)\n",
    "eeg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "In the data, the samples recorded are given a score from 0 to 128 based on how\n",
    "well-calibrated the sensor was (0 being best, 200 being worst). We filter the values\n",
    "based on an arbitrary cutoff limit of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def convert_string_data_to_values(value_string):\n",
    "    str_list = json.loads(value_string)\n",
    "    return str_list\n",
    "\n",
    "\n",
    "eeg[\"raw_values\"] = eeg[\"raw_values\"].apply(convert_string_data_to_values)\n",
    "\n",
    "eeg = eeg.loc[eeg[\"signal_quality\"] < QUALITY_THRESHOLD]\n",
    "eeg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Visualize one random sample from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We visualize one sample from the data to understand how the stimulus-induced signal looks\n",
    "like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def view_eeg_plot(idx):\n",
    "    data = eeg.loc[idx, \"raw_values\"]\n",
    "    plt.plot(data)\n",
    "    plt.title(f\"Sample random plot\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "view_eeg_plot(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Pre-process and collate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "There are a total of 67 different labels present in the data, where there are numbered\n",
    "sub-labels. We collate them under a single label as per their numbering and replace them\n",
    "in the data itself. Following this process, we perform simple Label encoding to get them\n",
    "in an integer format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "print(\"Before replacing labels\")\n",
    "print(eeg[\"label\"].unique(), \"\\n\")\n",
    "print(len(eeg[\"label\"].unique()), \"\\n\")\n",
    "\n",
    "\n",
    "eeg.replace(\n",
    "    {\n",
    "        \"label\": {\n",
    "            \"blink1\": \"blink\",\n",
    "            \"blink2\": \"blink\",\n",
    "            \"blink3\": \"blink\",\n",
    "            \"blink4\": \"blink\",\n",
    "            \"blink5\": \"blink\",\n",
    "            \"math1\": \"math\",\n",
    "            \"math2\": \"math\",\n",
    "            \"math3\": \"math\",\n",
    "            \"math4\": \"math\",\n",
    "            \"math5\": \"math\",\n",
    "            \"math6\": \"math\",\n",
    "            \"math7\": \"math\",\n",
    "            \"math8\": \"math\",\n",
    "            \"math9\": \"math\",\n",
    "            \"math10\": \"math\",\n",
    "            \"math11\": \"math\",\n",
    "            \"math12\": \"math\",\n",
    "            \"thinkOfItems-ver1\": \"thinkOfItems\",\n",
    "            \"thinkOfItems-ver2\": \"thinkOfItems\",\n",
    "            \"video-ver1\": \"video\",\n",
    "            \"video-ver2\": \"video\",\n",
    "            \"thinkOfItemsInstruction-ver1\": \"thinkOfItemsInstruction\",\n",
    "            \"thinkOfItemsInstruction-ver2\": \"thinkOfItemsInstruction\",\n",
    "            \"colorRound1-1\": \"colorRound1\",\n",
    "            \"colorRound1-2\": \"colorRound1\",\n",
    "            \"colorRound1-3\": \"colorRound1\",\n",
    "            \"colorRound1-4\": \"colorRound1\",\n",
    "            \"colorRound1-5\": \"colorRound1\",\n",
    "            \"colorRound1-6\": \"colorRound1\",\n",
    "            \"colorRound2-1\": \"colorRound2\",\n",
    "            \"colorRound2-2\": \"colorRound2\",\n",
    "            \"colorRound2-3\": \"colorRound2\",\n",
    "            \"colorRound2-4\": \"colorRound2\",\n",
    "            \"colorRound2-5\": \"colorRound2\",\n",
    "            \"colorRound2-6\": \"colorRound2\",\n",
    "            \"colorRound3-1\": \"colorRound3\",\n",
    "            \"colorRound3-2\": \"colorRound3\",\n",
    "            \"colorRound3-3\": \"colorRound3\",\n",
    "            \"colorRound3-4\": \"colorRound3\",\n",
    "            \"colorRound3-5\": \"colorRound3\",\n",
    "            \"colorRound3-6\": \"colorRound3\",\n",
    "            \"colorRound4-1\": \"colorRound4\",\n",
    "            \"colorRound4-2\": \"colorRound4\",\n",
    "            \"colorRound4-3\": \"colorRound4\",\n",
    "            \"colorRound4-4\": \"colorRound4\",\n",
    "            \"colorRound4-5\": \"colorRound4\",\n",
    "            \"colorRound4-6\": \"colorRound4\",\n",
    "            \"colorRound5-1\": \"colorRound5\",\n",
    "            \"colorRound5-2\": \"colorRound5\",\n",
    "            \"colorRound5-3\": \"colorRound5\",\n",
    "            \"colorRound5-4\": \"colorRound5\",\n",
    "            \"colorRound5-5\": \"colorRound5\",\n",
    "            \"colorRound5-6\": \"colorRound5\",\n",
    "            \"colorInstruction1\": \"colorInstruction\",\n",
    "            \"colorInstruction2\": \"colorInstruction\",\n",
    "            \"readyRound1\": \"readyRound\",\n",
    "            \"readyRound2\": \"readyRound\",\n",
    "            \"readyRound3\": \"readyRound\",\n",
    "            \"readyRound4\": \"readyRound\",\n",
    "            \"readyRound5\": \"readyRound\",\n",
    "            \"colorRound1\": \"colorRound\",\n",
    "            \"colorRound2\": \"colorRound\",\n",
    "            \"colorRound3\": \"colorRound\",\n",
    "            \"colorRound4\": \"colorRound\",\n",
    "            \"colorRound5\": \"colorRound\",\n",
    "        }\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "print(\"After replacing labels\")\n",
    "print(eeg[\"label\"].unique())\n",
    "print(len(eeg[\"label\"].unique()))\n",
    "\n",
    "le = preprocessing.LabelEncoder()  # Generates a look-up table\n",
    "le.fit(eeg[\"label\"])\n",
    "eeg[\"label\"] = le.transform(eeg[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We extract the number of unique classes present in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "num_classes = len(eeg[\"label\"].unique())\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We now visualize the number of samples present in each class using a Bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plt.bar(range(num_classes), eeg[\"label\"].value_counts())\n",
    "plt.title(\"Number of samples per class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Scale and split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We perform a simple Min-Max scaling to bring the value-range between 0 and 1. We do not\n",
    "use Standard Scaling as the data does not follow a Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "series_list = [\n",
    "    scaler.fit_transform(np.asarray(i).reshape(-1, 1)) for i in eeg[\"raw_values\"]\n",
    "]\n",
    "\n",
    "labels_list = [i for i in eeg[\"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We now create a Train-test split with a 15% holdout set. Following this, we reshape the\n",
    "data to create a sequence of length 512. We also convert the labels from their current\n",
    "label-encoded form to a one-hot encoding to enable use of several different\n",
    "`keras.metrics` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "    series_list, labels_list, test_size=0.15, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Length of x_train : {len(x_train)}\\nLength of x_test : {len(x_test)}\\nLength of y_train : {len(y_train)}\\nLength of y_test : {len(y_test)}\"\n",
    ")\n",
    "\n",
    "x_train = np.asarray(x_train).astype(np.float32).reshape(-1, 512, 1)\n",
    "y_train = np.asarray(y_train).astype(np.float32).reshape(-1, 1)\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "x_test = np.asarray(x_test).astype(np.float32).reshape(-1, 512, 1)\n",
    "y_test = np.asarray(y_test).astype(np.float32).reshape(-1, 1)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Prepare `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We now create a `tf.data.Dataset` from this data to prepare it for training. We also\n",
    "shuffle and batch the data for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Make Class Weights using Naive method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "As we can see from the plot of number of samples per class, the dataset is imbalanced.\n",
    "Hence, we **calculate weights for each class** to make sure that the model is trained in\n",
    "a fair manner without preference to any specific class due to greater number of samples.\n",
    "\n",
    "We use a naive method to calculate these weights, finding an **inverse proportion** of\n",
    "each class and using that as the weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "vals_dict = {}\n",
    "for i in eeg[\"label\"]:\n",
    "    if i in vals_dict.keys():\n",
    "        vals_dict[i] += 1\n",
    "    else:\n",
    "        vals_dict[i] = 1\n",
    "total = sum(vals_dict.values())\n",
    "\n",
    "# Formula used - Naive method where\n",
    "# weight = 1 - (no. of samples present / total no. of samples)\n",
    "# So more the samples, lower the weight\n",
    "\n",
    "weight_dict = {k: (1 - (v / total)) for k, v in vals_dict.items()}\n",
    "print(weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Define simple function to plot all the metrics present in a `keras.callbacks.History`\n",
    "object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_history_metrics(history: keras.callbacks.History):\n",
    "    total_plots = len(history.history)\n",
    "    cols = total_plots // 2\n",
    "\n",
    "    rows = total_plots // cols\n",
    "\n",
    "    if total_plots % cols != 0:\n",
    "        rows += 1\n",
    "\n",
    "    pos = range(1, total_plots + 1)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (key, value) in enumerate(history.history.items()):\n",
    "        plt.subplot(rows, cols, pos[i])\n",
    "        plt.plot(range(len(value)), value)\n",
    "        plt.title(str(key))\n",
    "    plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Define function to generate Convolutional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    input_layer = keras.Input(shape=(512, 1))\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        filters=32, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\"\n",
    "    )(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        filters=64, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\"\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        filters=128, kernel_size=5, strides=2, activation=\"relu\", padding=\"same\"\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        filters=256, kernel_size=5, strides=2, activation=\"relu\", padding=\"same\"\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        filters=512, kernel_size=7, strides=2, activation=\"relu\", padding=\"same\"\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        filters=1024,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        activation=\"relu\",\n",
    "        padding=\"same\",\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    x = layers.Dense(4096, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = layers.Dense(\n",
    "        2048, activation=\"relu\", kernel_regularizer=keras.regularizers.L2()\n",
    "    )(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = layers.Dense(\n",
    "        1024, activation=\"relu\", kernel_regularizer=keras.regularizers.L2()\n",
    "    )(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(\n",
    "        128, activation=\"relu\", kernel_regularizer=keras.regularizers.L2()\n",
    "    )(x)\n",
    "    output_layer = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return keras.Model(inputs=input_layer, outputs=output_layer)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Get Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "conv_model = create_model()\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Define callbacks, optimizer, loss and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We set the number of epochs at 30 after performing extensive experimentation. It was seen\n",
    "that this was the optimal number, after performing Early-Stopping analysis as well.\n",
    "We define a Model Checkpoint callback to make sure that we only get the best model\n",
    "weights.\n",
    "We also define a ReduceLROnPlateau as there were several cases found during\n",
    "experimentation where the loss stagnated after a certain point. On the other hand, a\n",
    "direct LRScheduler was found to be too aggressive in its decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model.keras\", save_best_only=True, monitor=\"loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_top_k_categorical_accuracy\",\n",
    "        factor=0.2,\n",
    "        patience=2,\n",
    "        min_lr=0.000001,\n",
    "    ),\n",
    "]\n",
    "\n",
    "optimizer = keras.optimizers.Adam(amsgrad=True, learning_rate=0.001)\n",
    "loss = keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Compile model and call `model.fit()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We use the `Adam` optimizer since it is commonly considered the best choice for\n",
    "preliminary training, and was found to be the best optimizer.\n",
    "We use `CategoricalCrossentropy` as the loss as our labels are in a one-hot-encoded form.\n",
    "\n",
    "We define the `TopKCategoricalAccuracy(k=3)`, `AUC`, `Precision` and `Recall` metrics to\n",
    "further aid in understanding the model better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "conv_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=[\n",
    "        keras.metrics.TopKCategoricalAccuracy(k=3),\n",
    "        keras.metrics.AUC(),\n",
    "        keras.metrics.Precision(),\n",
    "        keras.metrics.Recall(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "conv_model_history = conv_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=test_dataset,\n",
    "    class_weight=weight_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Visualize model metrics during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We use the function defined above to see model metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plot_history_metrics(conv_model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "loss, accuracy, auc, precision, recall = conv_model.evaluate(test_dataset)\n",
    "print(f\"Loss : {loss}\")\n",
    "print(f\"Top 3 Categorical Accuracy : {accuracy}\")\n",
    "print(f\"Area under the Curve (ROC) : {auc}\")\n",
    "print(f\"Precision : {precision}\")\n",
    "print(f\"Recall : {recall}\")\n",
    "\n",
    "\n",
    "def view_evaluated_eeg_plots(model):\n",
    "    start_index = random.randint(10, len(eeg))\n",
    "    end_index = start_index + 11\n",
    "    data = eeg.loc[start_index:end_index, \"raw_values\"]\n",
    "    data_array = [scaler.fit_transform(np.asarray(i).reshape(-1, 1)) for i in data]\n",
    "    data_array = [np.asarray(data_array).astype(np.float32).reshape(-1, 512, 1)]\n",
    "    original_labels = eeg.loc[start_index:end_index, \"label\"]\n",
    "    predicted_labels = np.argmax(model.predict(data_array, verbose=0), axis=1)\n",
    "    original_labels = [\n",
    "        le.inverse_transform(np.array(label).reshape(-1))[0]\n",
    "        for label in original_labels\n",
    "    ]\n",
    "    predicted_labels = [\n",
    "        le.inverse_transform(np.array(label).reshape(-1))[0]\n",
    "        for label in predicted_labels\n",
    "    ]\n",
    "    total_plots = 12\n",
    "    cols = total_plots // 3\n",
    "    rows = total_plots // cols\n",
    "    if total_plots % cols != 0:\n",
    "        rows += 1\n",
    "    pos = range(1, total_plots + 1)\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    for i, (plot_data, og_label, pred_label) in enumerate(\n",
    "        zip(data, original_labels, predicted_labels)\n",
    "    ):\n",
    "        plt.subplot(rows, cols, pos[i])\n",
    "        plt.plot(plot_data)\n",
    "        plt.title(f\"Actual Label : {og_label}\\nPredicted Label : {pred_label}\")\n",
    "        fig.subplots_adjust(hspace=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "view_evaluated_eeg_plots(conv_model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "eeg_signal_classification",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}