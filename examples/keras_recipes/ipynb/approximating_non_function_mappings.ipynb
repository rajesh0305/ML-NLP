{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Approximating non-Function Mappings with Mixture Density Networks\n",
    "\n",
    "**Author:** [lukewood](https://twitter.com/luke_wood_ml)<br>\n",
    "**Date created:** 2023/07/15<br>\n",
    "**Last modified:** 2023/07/15<br>\n",
    "**Description:** Approximate non one to one mapping using mixture density networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Approximating NonFunctions\n",
    "\n",
    "Neural networks are universal function approximators. Key word: function!\n",
    "While powerful function approximators, neural networks are not able to\n",
    "approximate non-functions.\n",
    "One important restriction to remember about functions - they have one input, one\n",
    "output!\n",
    "Neural networks suffer greatly when the training set has multiple values of Y for a single X.\n",
    "\n",
    "In this guide I'll show you how to approximate the class of non-functions\n",
    "consisting of mappings from `x -> y` such that multiple `y` may exist for a\n",
    "given `x`.  We'll use a class of neural networks called\n",
    "\"Mixture Density Networks\".\n",
    "\n",
    "I'm going to use the new\n",
    "[multibackend Keras V3](https://github.com/keras-team/keras) to\n",
    "build my Mixture Density networks.\n",
    "Great job to the Keras team on the project - it's awesome to be able to swap\n",
    "frameworks in one line of code.\n",
    "\n",
    "Some bad news: I use TensorFlow probability in this guide... so it\n",
    "actually works only with TensorFlow and JAX backends.\n",
    "\n",
    "Anyways, let's start by installing dependencies and sorting out imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env KERAS_BACKEND=jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "%pip install -q --upgrade jax tensorflow-probability[jax] keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import callbacks, layers, ops\n",
    "from tensorflow_probability.substrates.jax import distributions as tfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Next, lets generate a noisy spiral that we're going to attempt to approximate.\n",
    "I've defined a few functions below to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def normalize(x):\n",
    "    return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "\n",
    "def create_noisy_spiral(n, jitter_std=0.2, revolutions=2):\n",
    "    angle = np.random.uniform(0, 2 * np.pi * revolutions, [n])\n",
    "    r = angle\n",
    "\n",
    "    x = r * np.cos(angle)\n",
    "    y = r * np.sin(angle)\n",
    "\n",
    "    result = np.stack([x, y], axis=1)\n",
    "    result = result + np.random.normal(scale=jitter_std, size=[n, 2])\n",
    "    result = 5 * normalize(result)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Next, lets invoke this function many times to construct a sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "xy = create_noisy_spiral(10000)\n",
    "\n",
    "x, y = xy[:, 0:1], xy[:, 1:]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "As you can see, there's multiple possible values for Y with respect to a given\n",
    "X.  Normal neural networks will simply learn the mean of these points with\n",
    "respect to geometric space.\n",
    "\n",
    "We can quickly show this with a simple linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "N_HIDDEN = 128\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(N_HIDDEN, activation=\"relu\"),\n",
    "        layers.Dense(N_HIDDEN, activation=\"relu\"),\n",
    "        layers.Dense(1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's use mean squared error as well as the adam optimizer.\n",
    "These tend to be reasonable prototyping choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We can fit this model quite easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x,\n",
    "    y,\n",
    "    epochs=300,\n",
    "    batch_size=128,\n",
    "    validation_split=0.15,\n",
    "    callbacks=[callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "And let's check out the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "As expected, the model learns the geometric mean of all points in `y` for a\n",
    "given `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.scatter(x, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Mixture Density Networks\n",
    "\n",
    "Mixture Density networks can alleviate this problem.\n",
    "A Mixture density is a class of complicated densities expressible in terms of simpler densities.\n",
    "They are effectively the sum of a ton of probability distributions.\n",
    "Mixture Density networks learn to parameterize a mixture density distribution\n",
    "based on a given training set.\n",
    "\n",
    "As a practitioner, all you need to know, is that Mixture Density Networks solve\n",
    "the problem of multiple values of Y for a given X.\n",
    "I'm hoping to add a tool to your kit- but I'm not going to formally explain the\n",
    "derivation of Mixture Density networks in this guide.\n",
    "The most important thing to know is that a Mixture Density network learns to\n",
    "parameterize a mixture density distribution.\n",
    "This is done by computing a special loss with respect to both the provided\n",
    "`y_i` label as well as the predicted distribution for the corresponding `x_i`.\n",
    "This loss function operates by computing the probability that `y_i` would be\n",
    "drawn from the predicted mixture distribution.\n",
    "\n",
    "Let's implement a Mixture density network.\n",
    "Below, a ton of helper functions are defined based on an old Keras library\n",
    "[`Keras Mixture Density Network Layer`](https://github.com/cpmpercussion/keras-mdn-layer).\n",
    "\n",
    "I've adapted the code for use with Keras core.\n",
    "\n",
    "Lets start writing a Mixture Density Network!\n",
    "First, we need a special activation function: ELU plus a tiny epsilon.\n",
    "This helps prevent ELU from outputting 0 which causes NaNs in Mixture Density\n",
    "Network loss evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def elu_plus_one_plus_epsilon(x):\n",
    "    return keras.activations.elu(x) + 1 + keras.backend.epsilon()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Next, lets actually define a MixtureDensity layer that outputs all values needed\n",
    "to sample from the learned mixture distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MixtureDensityOutput(layers.Layer):\n",
    "    def __init__(self, output_dimension, num_mixtures, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dim = output_dimension\n",
    "        self.num_mix = num_mixtures\n",
    "        self.mdn_mus = layers.Dense(\n",
    "            self.num_mix * self.output_dim, name=\"mdn_mus\"\n",
    "        )  # mix*output vals, no activation\n",
    "        self.mdn_sigmas = layers.Dense(\n",
    "            self.num_mix * self.output_dim,\n",
    "            activation=elu_plus_one_plus_epsilon,\n",
    "            name=\"mdn_sigmas\",\n",
    "        )  # mix*output vals exp activation\n",
    "        self.mdn_pi = layers.Dense(self.num_mix, name=\"mdn_pi\")  # mix vals, logits\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mdn_mus.build(input_shape)\n",
    "        self.mdn_sigmas.build(input_shape)\n",
    "        self.mdn_pi.build(input_shape)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    @property\n",
    "    def trainable_weights(self):\n",
    "        return (\n",
    "            self.mdn_mus.trainable_weights\n",
    "            + self.mdn_sigmas.trainable_weights\n",
    "            + self.mdn_pi.trainable_weights\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def non_trainable_weights(self):\n",
    "        return (\n",
    "            self.mdn_mus.non_trainable_weights\n",
    "            + self.mdn_sigmas.non_trainable_weights\n",
    "            + self.mdn_pi.non_trainable_weights\n",
    "        )\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return layers.concatenate(\n",
    "            [self.mdn_mus(x), self.mdn_sigmas(x), self.mdn_pi(x)], name=\"mdn_outputs\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Lets construct an Mixture Density Network using our new layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIMS = 1\n",
    "N_MIXES = 20\n",
    "\n",
    "mdn_network = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(N_HIDDEN, activation=\"relu\"),\n",
    "        layers.Dense(N_HIDDEN, activation=\"relu\"),\n",
    "        MixtureDensityOutput(OUTPUT_DIMS, N_MIXES),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Next, let's implement a custom loss function to train the Mixture Density\n",
    "Network layer based on the true values and our expected outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def get_mixture_loss_func(output_dim, num_mixes):\n",
    "    def mdn_loss_func(y_true, y_pred):\n",
    "        # Reshape inputs in case this is used in a TimeDistributed layer\n",
    "        y_pred = ops.reshape(y_pred, [-1, (2 * num_mixes * output_dim) + num_mixes])\n",
    "        y_true = ops.reshape(y_true, [-1, output_dim])\n",
    "        # Split the inputs into parameters\n",
    "        out_mu, out_sigma, out_pi = ops.split(y_pred, 3, axis=-1)\n",
    "        # Construct the mixture models\n",
    "        cat = tfd.Categorical(logits=out_pi)\n",
    "        mus = ops.split(out_mu, num_mixes, axis=1)\n",
    "        sigs = ops.split(out_sigma, num_mixes, axis=1)\n",
    "        coll = [\n",
    "            tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale)\n",
    "            for loc, scale in zip(mus, sigs)\n",
    "        ]\n",
    "        mixture = tfd.Mixture(cat=cat, components=coll)\n",
    "        loss = mixture.log_prob(y_true)\n",
    "        loss = ops.negative(loss)\n",
    "        loss = ops.mean(loss)\n",
    "        return loss\n",
    "\n",
    "    return mdn_loss_func\n",
    "\n",
    "\n",
    "mdn_network.compile(loss=get_mixture_loss_func(OUTPUT_DIMS, N_MIXES), optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Finally, we can call `model.fit()` like any other Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "mdn_network.fit(\n",
    "    x,\n",
    "    y,\n",
    "    epochs=300,\n",
    "    batch_size=128,\n",
    "    validation_split=0.15,\n",
    "    callbacks=[\n",
    "        callbacks.EarlyStopping(monitor=\"loss\", patience=10, restore_best_weights=True),\n",
    "        callbacks.ReduceLROnPlateau(monitor=\"loss\", patience=5),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's make some predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "y_pred_mixture = mdn_network.predict(x)\n",
    "print(y_pred_mixture.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The MDN does not output a single value; instead it outputs values to\n",
    "parameterize a mixture distribution.\n",
    "To visualize these outputs, lets sample from the distribution.\n",
    "\n",
    "Note that sampling is a lossy process.\n",
    "If you want to preserve all information as part of a greater latent\n",
    "representation (i.e. for downstream processing) I recommend you simply keep the\n",
    "distribution parameters in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_mixture_params(params, output_dim, num_mixes):\n",
    "    mus = params[: num_mixes * output_dim]\n",
    "    sigs = params[num_mixes * output_dim : 2 * num_mixes * output_dim]\n",
    "    pi_logits = params[-num_mixes:]\n",
    "    return mus, sigs, pi_logits\n",
    "\n",
    "\n",
    "def softmax(w, t=1.0):\n",
    "    e = np.array(w) / t  # adjust temperature\n",
    "    e -= e.max()  # subtract max to protect from exploding exp values.\n",
    "    e = np.exp(e)\n",
    "    dist = e / np.sum(e)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def sample_from_categorical(dist):\n",
    "    r = np.random.rand(1)  # uniform random number in [0,1]\n",
    "    accumulate = 0\n",
    "    for i in range(0, dist.size):\n",
    "        accumulate += dist[i]\n",
    "        if accumulate >= r:\n",
    "            return i\n",
    "    print(\"Error sampling categorical model.\")\n",
    "    return -1\n",
    "\n",
    "\n",
    "def sample_from_output(params, output_dim, num_mixes, temp=1.0, sigma_temp=1.0):\n",
    "    mus, sigs, pi_logits = split_mixture_params(params, output_dim, num_mixes)\n",
    "    pis = softmax(pi_logits, t=temp)\n",
    "    m = sample_from_categorical(pis)\n",
    "    # Alternative way to sample from categorical:\n",
    "    # m = np.random.choice(range(len(pis)), p=pis)\n",
    "    mus_vector = mus[m * output_dim : (m + 1) * output_dim]\n",
    "    sig_vector = sigs[m * output_dim : (m + 1) * output_dim]\n",
    "    scale_matrix = np.identity(output_dim) * sig_vector  # scale matrix from diag\n",
    "    cov_matrix = np.matmul(scale_matrix, scale_matrix.T)  # cov is scale squared.\n",
    "    cov_matrix = cov_matrix * sigma_temp  # adjust for sigma temperature\n",
    "    sample = np.random.multivariate_normal(mus_vector, cov_matrix, 1)\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Next lets use our sampling function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Sample from the predicted distributions\n",
    "y_samples = np.apply_along_axis(\n",
    "    sample_from_output, 1, y_pred_mixture, 1, N_MIXES, temp=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Finally, we can visualize our network outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y, alpha=0.05, color=\"blue\", label=\"Ground Truth\")\n",
    "plt.scatter(\n",
    "    x,\n",
    "    y_samples[:, :, 0],\n",
    "    color=\"green\",\n",
    "    alpha=0.05,\n",
    "    label=\"Mixture Density Network prediction\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Beautiful.  Love to see it\n",
    "\n",
    "# Conclusions\n",
    "\n",
    "Neural Networks are universal function approximators - but they can only\n",
    "approximate functions.  Mixture Density networks can approximate arbitrary\n",
    "x->y mappings using some neat probability tricks.\n",
    "\n",
    "For more examples with `tensorflow_probability`\n",
    "[start here](https://www.tensorflow.org/probability/examples/Probabilistic_Layers_Regression).\n",
    "\n",
    "One more pretty graphic for the road:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.set_figheight(3)\n",
    "fig.set_figwidth(12)\n",
    "axs[0].set_title(\"Ground Truth\")\n",
    "axs[0].scatter(x, y, alpha=0.05, color=\"blue\")\n",
    "xlim = axs[0].get_xlim()\n",
    "ylim = axs[0].get_ylim()\n",
    "\n",
    "axs[1].set_title(\"Normal Model prediction\")\n",
    "axs[1].scatter(x, y_pred, alpha=0.05, color=\"red\")\n",
    "axs[1].set_xlim(xlim)\n",
    "axs[1].set_ylim(ylim)\n",
    "axs[2].scatter(\n",
    "    x,\n",
    "    y_samples[:, :, 0],\n",
    "    color=\"green\",\n",
    "    alpha=0.05,\n",
    "    label=\"Mixture Density Network prediction\",\n",
    ")\n",
    "axs[2].set_title(\"Mixture Density Network prediction\")\n",
    "axs[2].set_xlim(xlim)\n",
    "axs[2].set_ylim(ylim)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "None",
  "colab": {
   "collapsed_sections": [],
   "name": "approximating_non_function_mappings",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
