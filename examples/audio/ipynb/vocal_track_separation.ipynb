{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Vocal Track Separation with Encoder-Decoder Architecture\n",
    "\n",
    "**Author:** [Joaquin Jimenez](https://github.com/johacks/)<br>\n",
    "**Date created:** 2024/12/10<br>\n",
    "**Last modified:** 2024/12/10<br>\n",
    "**Description:** Train a model to separate vocal tracks from music mixtures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we build a vocal track separation model using an encoder-decoder\n",
    "architecture in Keras 3.\n",
    "\n",
    "We train the model on the [MUSDB18 dataset](https://doi.org/10.5281/zenodo.1117372),\n",
    "which provides music mixtures and isolated tracks for drums, bass, other, and vocals.\n",
    "\n",
    "Key concepts covered:\n",
    "\n",
    "- Audio data preprocessing using the Short-Time Fourier Transform (STFT).\n",
    "- Audio data augmentation techniques.\n",
    "- Implementing custom encoders and decoders specialized for audio data.\n",
    "- Defining appropriate loss functions and metrics for audio source separation tasks.\n",
    "\n",
    "The model architecture is derived from the TFC_TDF_Net model described in:\n",
    "\n",
    "W. Choi, M. Kim, J. Chung, D. Lee, and S. Jung, \u201cInvestigating U-Nets with various\n",
    "intermediate blocks for spectrogram-based singing voice separation,\u201d in the 21st\n",
    "International Society for Music Information Retrieval Conference, 2020.\n",
    "\n",
    "For reference code, see:\n",
    "[GitHub: ws-choi/ISMIR2020_U_Nets_SVS](https://github.com/ws-choi/ISMIR2020_U_Nets_SVS).\n",
    "\n",
    "The data processing and model training routines are partly derived from:\n",
    "[ZFTurbo/Music-Source-Separation-Training](https://github.com/ZFTurbo/Music-Source-Separation-Training/tree/main)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Import and install all the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install -qq audiomentations soundfile ffmpeg-binaries\n",
    "!pip install -qq \"keras==3.7.0\"\n",
    "!sudo -n apt-get install -y graphviz >/dev/null 2>&1  # Required for plotting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
    "\n",
    "import random\n",
    "import subprocess\n",
    "import tempfile\n",
    "import typing\n",
    "from os import path\n",
    "\n",
    "import audiomentations as aug\n",
    "import ffmpeg\n",
    "import keras\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from IPython import display\n",
    "from keras import callbacks, layers, ops, saving\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Configuration\n",
    "\n",
    "The following constants define configuration parameters for audio processing\n",
    "and model training, including dataset paths, audio chunk sizes, Short-Time Fourier\n",
    "Transform (STFT) parameters, and training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# MUSDB18 dataset configuration\n",
    "MUSDB_STREAMS = {\"mixture\": 0, \"drums\": 1, \"bass\": 2, \"other\": 3, \"vocals\": 4}\n",
    "TARGET_INSTRUMENTS = {track: MUSDB_STREAMS[track] for track in (\"vocals\",)}\n",
    "N_INSTRUMENTS = len(TARGET_INSTRUMENTS)\n",
    "SOURCE_INSTRUMENTS = tuple(k for k in MUSDB_STREAMS if k != \"mixture\")\n",
    "\n",
    "# Audio preprocessing parameters for Short-Time Fourier Transform (STFT)\n",
    "N_SUBBANDS = 4  # Number of subbands into which frequencies are split\n",
    "CHUNK_SIZE = 65024  # Number of amplitude samples per audio chunk (~4 seconds)\n",
    "STFT_N_FFT = 2048  # FFT points used in STFT\n",
    "STFT_HOP_LENGTH = 512  # Hop length for STFT\n",
    "\n",
    "# Training hyperparameters\n",
    "N_CHANNELS = 64  # Base channel count for the model\n",
    "BATCH_SIZE = 3\n",
    "ACCUMULATION_STEPS = 2\n",
    "EFFECTIVE_BATCH_SIZE = BATCH_SIZE * (ACCUMULATION_STEPS or 1)\n",
    "\n",
    "# Paths\n",
    "TMP_DIR = path.expanduser(\"~/.keras/tmp\")\n",
    "DATASET_DIR = path.expanduser(\"~/.keras/datasets\")\n",
    "MODEL_PATH = path.join(TMP_DIR, f\"model_{keras.backend.backend()}.keras\")\n",
    "CSV_LOG_PATH = path.join(TMP_DIR, f\"training_{keras.backend.backend()}.csv\")\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "keras.utils.set_random_seed(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## MUSDB18 Dataset\n",
    "\n",
    "The MUSDB18 dataset is a standard benchmark for music source separation, containing\n",
    "150 full-length music tracks along with isolated drums, bass, other, and vocals.\n",
    "The dataset is stored in .mp4 format, and each .mp4 file includes multiple audio\n",
    "streams (mixture and individual tracks).\n",
    "\n",
    "### Download and Conversion\n",
    "\n",
    "The following utility function downloads MUSDB18 and converts its .mp4 files to\n",
    ".wav files for each instrument track, resampled to 16 kHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def download_musdb18(out_dir=None):\n",
    "    \"\"\"Download and extract the MUSDB18 dataset, then convert .mp4 files to .wav files.\n",
    "\n",
    "    MUSDB18 reference:\n",
    "    Rafii, Z., Liutkus, A., St\u00f6ter, F.-R., Mimilakis, S. I., & Bittner, R. (2017).\n",
    "    MUSDB18 - a corpus for music separation (1.0.0) [Data set]. Zenodo.\n",
    "    \"\"\"\n",
    "    ffmpeg.init()\n",
    "    from ffmpeg import FFMPEG_PATH\n",
    "\n",
    "    # Create output directories\n",
    "    os.makedirs((base := out_dir or tempfile.mkdtemp()), exist_ok=True)\n",
    "    if path.exists((out_dir := path.join(base, \"musdb18_wav\"))):\n",
    "        print(\"MUSDB18 dataset already downloaded\")\n",
    "        return out_dir\n",
    "\n",
    "    # Download and extract the dataset\n",
    "    download_dir = keras.utils.get_file(\n",
    "        fname=\"musdb18\",\n",
    "        origin=\"https://zenodo.org/records/1117372/files/musdb18.zip\",\n",
    "        extract=True,\n",
    "    )\n",
    "\n",
    "    # ffmpeg command template: input, stream index, output\n",
    "    ffmpeg_args = str(FFMPEG_PATH) + \" -v error -i {} -map 0:{} -vn -ar 16000 {}\"\n",
    "\n",
    "    # Convert each mp4 file to multiple .wav files for each track\n",
    "    for split in (\"train\", \"test\"):\n",
    "        songs = os.listdir(path.join(download_dir, split))\n",
    "        for i, song in enumerate(songs):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"{split.capitalize()}: {i}/{len(songs)} songs processed\")\n",
    "\n",
    "            mp4_path_orig = path.join(download_dir, split, song)\n",
    "            mp4_path = path.join(tempfile.mkdtemp(), split, song.replace(\" \", \"_\"))\n",
    "            os.makedirs(path.dirname(mp4_path), exist_ok=True)\n",
    "            os.rename(mp4_path_orig, mp4_path)\n",
    "\n",
    "            wav_dir = path.join(out_dir, split, path.basename(mp4_path).split(\".\")[0])\n",
    "            os.makedirs(wav_dir, exist_ok=True)\n",
    "\n",
    "            for track in SOURCE_INSTRUMENTS:\n",
    "                out_path = path.join(wav_dir, f\"{track}.wav\")\n",
    "                stream_index = MUSDB_STREAMS[track]\n",
    "                args = ffmpeg_args.format(mp4_path, stream_index, out_path).split()\n",
    "                assert subprocess.run(args).returncode == 0, \"ffmpeg conversion failed\"\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "# Download and prepare the MUSDB18 dataset\n",
    "songs = download_musdb18(out_dir=DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Custom Dataset\n",
    "\n",
    "We define a custom dataset class to generate random audio chunks and their corresponding\n",
    "labels. The dataset does the following:\n",
    "\n",
    "1. Selects a random chunk from a random song and instrument.\n",
    "2. Applies optional data augmentations.\n",
    "3. Combines isolated tracks to form new synthetic mixtures.\n",
    "4. Prepares features (mixtures) and labels (vocals) for training.\n",
    "\n",
    "This approach allows creating an effectively infinite variety of training examples\n",
    "through randomization and augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Dataset(keras.utils.PyDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        songs,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        batches_per_epoch=1000 * ACCUMULATION_STEPS,\n",
    "        augmentation=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.augmentation = augmentation\n",
    "        self.vocals_augmentations = [\n",
    "            aug.PitchShift(min_semitones=-5, max_semitones=5, p=0.1),\n",
    "            aug.SevenBandParametricEQ(-9, 9, p=0.25),\n",
    "            aug.TanhDistortion(0.1, 0.7, p=0.1),\n",
    "        ]\n",
    "        self.other_augmentations = [\n",
    "            aug.PitchShift(p=0.1),\n",
    "            aug.AddGaussianNoise(p=0.1),\n",
    "        ]\n",
    "        self.songs = songs\n",
    "        self.sizes = {song: self.get_track_set_size(song) for song in self.songs}\n",
    "        self.batch_size = batch_size\n",
    "        self.chunk_size = chunk_size\n",
    "        self.batches_per_epoch = batches_per_epoch\n",
    "\n",
    "    def get_track_set_size(self, song: str):\n",
    "        \"\"\"Return the smallest track length in the given song directory.\"\"\"\n",
    "        sizes = [len(sf.read(p)[0]) for p in glob.glob(path.join(song, \"*.wav\"))]\n",
    "        if max(sizes) != min(sizes):\n",
    "            print(f\"Warning: {song} has different track lengths\")\n",
    "        return min(sizes)\n",
    "\n",
    "    def random_chunk_of_instrument_type(self, instrument: str):\n",
    "        \"\"\"Extract a random chunk for the specified instrument from a random song.\"\"\"\n",
    "        song, size = random.choice(list(self.sizes.items()))\n",
    "        track = path.join(song, f\"{instrument}.wav\")\n",
    "\n",
    "        if self.chunk_size <= size:\n",
    "            start = np.random.randint(size - self.chunk_size + 1)\n",
    "            audio = sf.read(track, self.chunk_size, start, dtype=\"float32\")[0]\n",
    "            audio_mono = np.mean(audio, axis=1)\n",
    "        else:\n",
    "            # If the track is shorter than chunk_size, pad the signal\n",
    "            audio_mono = np.mean(sf.read(track, dtype=\"float32\")[0], axis=1)\n",
    "            audio_mono = np.pad(audio_mono, ((0, self.chunk_size - size),))\n",
    "\n",
    "        # If the chunk is almost silent, retry\n",
    "        if np.mean(np.abs(audio_mono)) < 0.01:\n",
    "            return self.random_chunk_of_instrument_type(instrument)\n",
    "\n",
    "        return self.data_augmentation(audio_mono, instrument)\n",
    "\n",
    "    def data_augmentation(self, audio: np.ndarray, instrument: str):\n",
    "        \"\"\"Apply data augmentation to the audio chunk, if enabled.\"\"\"\n",
    "\n",
    "        def coin_flip(x, probability: float, fn: typing.Callable):\n",
    "            return fn(x) if random.uniform(0, 1) < probability else x\n",
    "\n",
    "        if self.augmentation:\n",
    "            augmentations = (\n",
    "                self.vocals_augmentations\n",
    "                if instrument == \"vocals\"\n",
    "                else self.other_augmentations\n",
    "            )\n",
    "            # Loudness augmentation\n",
    "            audio *= np.random.uniform(0.5, 1.5, (len(audio),)).astype(\"float32\")\n",
    "            # Random reverse\n",
    "            audio = coin_flip(audio, 0.1, lambda x: np.flip(x))\n",
    "            # Random polarity inversion\n",
    "            audio = coin_flip(audio, 0.5, lambda x: -x)\n",
    "            # Apply selected augmentations\n",
    "            for aug_ in augmentations:\n",
    "                aug_.randomize_parameters(audio, sample_rate=16000)\n",
    "                audio = aug_(audio, sample_rate=16000)\n",
    "        return audio\n",
    "\n",
    "    def random_mix_of_tracks(self) -> dict:\n",
    "        \"\"\"Create a random mix of instruments by summing their individual chunks.\"\"\"\n",
    "        tracks = {}\n",
    "        for instrument in SOURCE_INSTRUMENTS:\n",
    "            # Start with a single random chunk\n",
    "            mixup = [self.random_chunk_of_instrument_type(instrument)]\n",
    "\n",
    "            # Randomly add more chunks of the same instrument (mixup augmentation)\n",
    "            if self.augmentation:\n",
    "                for p in (0.2, 0.02):\n",
    "                    if random.uniform(0, 1) < p:\n",
    "                        mixup.append(self.random_chunk_of_instrument_type(instrument))\n",
    "\n",
    "            tracks[instrument] = np.mean(mixup, axis=0, dtype=\"float32\")\n",
    "        return tracks\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batches_per_epoch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate a batch of random mixtures\n",
    "        batch = [self.random_mix_of_tracks() for _ in range(self.batch_size)]\n",
    "\n",
    "        # Features: sum of all tracks\n",
    "        batch_x = ops.sum(\n",
    "            np.array([list(track_set.values()) for track_set in batch]), axis=1\n",
    "        )\n",
    "\n",
    "        # Labels: isolated target instruments (e.g., vocals)\n",
    "        batch_y = np.array(\n",
    "            [[track_set[t] for t in TARGET_INSTRUMENTS] for track_set in batch]\n",
    "        )\n",
    "\n",
    "        return batch_x, ops.convert_to_tensor(batch_y)\n",
    "\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_ds = Dataset(glob.glob(path.join(songs, \"train\", \"*\")))\n",
    "val_ds = Dataset(\n",
    "    glob.glob(path.join(songs, \"test\", \"*\")),\n",
    "    batches_per_epoch=int(0.1 * train_ds.batches_per_epoch),\n",
    "    augmentation=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Visualize a Sample\n",
    "\n",
    "Let's visualize a random mixed audio chunk and its corresponding isolated vocals.\n",
    "This helps to understand the nature of the preprocessed input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def visualize_audio_np(audio: np.ndarray, rate=16000, name=\"mixup\"):\n",
    "    \"\"\"Plot and display an audio waveform and also produce an Audio widget.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(audio)\n",
    "    plt.title(f\"Waveform: {name}\")\n",
    "    plt.xlim(0, len(audio))\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()\n",
    "    # plt.savefig(f\"tmp/{name}.png\")\n",
    "\n",
    "    # Normalize and display audio\n",
    "    audio_norm = (audio - np.min(audio)) / (np.max(audio) - np.min(audio) + 1e-8)\n",
    "    audio_norm = (audio_norm * 2 - 1) * 0.6\n",
    "    display.display(display.Audio(audio_norm, rate=rate))\n",
    "    # sf.write(f\"tmp/{name}.wav\", audio_norm, rate)\n",
    "\n",
    "\n",
    "sample_batch_x, sample_batch_y = val_ds[None]  # Random batch\n",
    "visualize_audio_np(ops.convert_to_numpy(sample_batch_x[0]))\n",
    "visualize_audio_np(ops.convert_to_numpy(sample_batch_y[0, 0]), name=\"vocals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Model\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "The model operates on STFT representations rather than raw audio. We define a\n",
    "preprocessing model to compute STFT and a corresponding inverse transform (iSTFT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def stft(inputs, fft_size=STFT_N_FFT, sequence_stride=STFT_HOP_LENGTH):\n",
    "    \"\"\"Compute the STFT for the input audio and return the real and imaginary parts.\"\"\"\n",
    "    real_x, imag_x = ops.stft(inputs, fft_size, sequence_stride, fft_size)\n",
    "    real_x, imag_x = ops.expand_dims(real_x, -1), ops.expand_dims(imag_x, -1)\n",
    "    x = ops.concatenate((real_x, imag_x), axis=-1)\n",
    "\n",
    "    # Drop last freq sample for convenience\n",
    "    return ops.split(x, [x.shape[2] - 1], axis=2)[0]\n",
    "\n",
    "\n",
    "def inverse_stft(inputs, fft_size=STFT_N_FFT, sequence_stride=STFT_HOP_LENGTH):\n",
    "    \"\"\"Compute the inverse STFT for the given STFT input.\"\"\"\n",
    "    x = inputs\n",
    "\n",
    "    # Pad back dropped freq sample if using torch backend\n",
    "    if keras.backend.backend() == \"torch\":\n",
    "        x = ops.pad(x, ((0, 0), (0, 0), (0, 1), (0, 0)))\n",
    "\n",
    "    real_x, imag_x = ops.split(x, 2, axis=-1)\n",
    "    real_x = ops.squeeze(real_x, axis=-1)\n",
    "    imag_x = ops.squeeze(imag_x, axis=-1)\n",
    "\n",
    "    return ops.istft((real_x, imag_x), fft_size, sequence_stride, fft_size)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses a custom encoder-decoder architecture with Time-Frequency Convolution\n",
    "(TFC) and Time-Distributed Fully Connected (TDF) blocks. They are grouped into a\n",
    "`TimeFrequencyTransformBlock`, i.e. \"TFC_TDF\" in the original paper by Choi et al.\n",
    "\n",
    "We then define an encoder-decoder network with multiple scales. Each encoder scale\n",
    "applies TFC_TDF blocks followed by downsampling, while decoder scales apply TFC_TDF\n",
    "blocks over the concatenation of upsampled features and associated encoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "@saving.register_keras_serializable()\n",
    "class TimeDistributedDenseBlock(layers.Layer):\n",
    "    \"\"\"Time-Distributed Fully Connected layer block.\n",
    "\n",
    "    Applies frequency-wise dense transformations across time frames with instance\n",
    "    normalization and GELU activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bottleneck_factor, fft_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.fft_dim = fft_dim\n",
    "        self.hidden_dim = fft_dim // bottleneck_factor\n",
    "\n",
    "    def build(self, *_):\n",
    "        self.group_norm_1 = layers.GroupNormalization(groups=-1)\n",
    "        self.group_norm_2 = layers.GroupNormalization(groups=-1)\n",
    "        self.dense_1 = layers.Dense(self.hidden_dim, use_bias=False)\n",
    "        self.dense_2 = layers.Dense(self.fft_dim, use_bias=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Apply normalization and dense layers frequency-wise\n",
    "        x = ops.gelu(self.group_norm_1(x))\n",
    "        x = ops.swapaxes(x, -1, -2)\n",
    "        x = self.dense_1(x)\n",
    "\n",
    "        x = ops.gelu(self.group_norm_2(ops.swapaxes(x, -1, -2)))\n",
    "        x = ops.swapaxes(x, -1, -2)\n",
    "        x = self.dense_2(x)\n",
    "        return ops.swapaxes(x, -1, -2)\n",
    "\n",
    "\n",
    "@saving.register_keras_serializable()\n",
    "class TimeFrequencyConvolution(layers.Layer):\n",
    "    \"\"\"Time-Frequency Convolutional layer.\n",
    "\n",
    "    Applies a 2D convolution over time-frequency representations and applies instance\n",
    "    normalization and GELU activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.channels = channels\n",
    "\n",
    "    def build(self, *_):\n",
    "        self.group_norm = layers.GroupNormalization(groups=-1)\n",
    "        self.conv = layers.Conv2D(self.channels, 3, padding=\"same\", use_bias=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.conv(ops.gelu(self.group_norm(x)))\n",
    "\n",
    "\n",
    "@saving.register_keras_serializable()\n",
    "class TimeFrequencyTransformBlock(layers.Layer):\n",
    "    \"\"\"Implements TFC_TDF block for encoder-decoder architecture.\n",
    "\n",
    "    Repeatedly apply Time-Frequency Convolution and Time-Distributed Dense blocks as\n",
    "    many times as specified by the `length` parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, channels, length, fft_dim, bottleneck_factor, in_channels=None, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.channels = channels\n",
    "        self.length = length\n",
    "        self.fft_dim = fft_dim\n",
    "        self.bottleneck_factor = bottleneck_factor\n",
    "        self.in_channels = in_channels or channels\n",
    "        self.blocks = []\n",
    "\n",
    "    def build(self, *_):\n",
    "        # Add blocks in a flat list to avoid nested structures\n",
    "        for i in range(self.length):\n",
    "            in_channels = self.channels if i > 0 else self.in_channels\n",
    "            self.blocks.append(TimeFrequencyConvolution(in_channels))\n",
    "            self.blocks.append(\n",
    "                TimeDistributedDenseBlock(self.bottleneck_factor, self.fft_dim)\n",
    "            )\n",
    "            self.blocks.append(TimeFrequencyConvolution(self.channels))\n",
    "            # Residual connection\n",
    "            self.blocks.append(layers.Conv2D(self.channels, 1, 1, use_bias=False))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        # Each block consists of 4 layers:\n",
    "        # 1. Time-Frequency Convolution\n",
    "        # 2. Time-Distributed Dense\n",
    "        # 3. Time-Frequency Convolution\n",
    "        # 4. Residual connection\n",
    "        for i in range(0, len(self.blocks), 4):\n",
    "            tfc_1 = self.blocks[i](x)\n",
    "            tdf = self.blocks[i + 1](x)\n",
    "            tfc_2 = self.blocks[i + 2](tfc_1 + tdf)\n",
    "            x = tfc_2 + self.blocks[i + 3](x)  # Residual connection\n",
    "        return x\n",
    "\n",
    "\n",
    "@saving.register_keras_serializable()\n",
    "class Downscale(layers.Layer):\n",
    "    \"\"\"Downscale time-frequency dimensions using a convolution.\"\"\"\n",
    "\n",
    "    conv_cls = layers.Conv2D\n",
    "\n",
    "    def __init__(self, channels, scale, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.channels = channels\n",
    "        self.scale = scale\n",
    "\n",
    "    def build(self, *_):\n",
    "        self.conv = self.conv_cls(self.channels, self.scale, self.scale, use_bias=False)\n",
    "        self.norm = layers.GroupNormalization(groups=-1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.norm(ops.gelu(self.conv(inputs)))\n",
    "\n",
    "\n",
    "@saving.register_keras_serializable()\n",
    "class Upscale(Downscale):\n",
    "    \"\"\"Upscale time-frequency dimensions using a transposed convolution.\"\"\"\n",
    "\n",
    "    conv_cls = layers.Conv2DTranspose\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    inputs,\n",
    "    n_instruments=N_INSTRUMENTS,\n",
    "    n_subbands=N_SUBBANDS,\n",
    "    channels=N_CHANNELS,\n",
    "    fft_dim=(STFT_N_FFT // 2) // N_SUBBANDS,\n",
    "    n_scales=4,\n",
    "    scale=(2, 2),\n",
    "    block_size=2,\n",
    "    growth=128,\n",
    "    bottleneck_factor=2,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Build the TFC_TDF encoder-decoder model for source separation.\"\"\"\n",
    "    # Compute STFT\n",
    "    x = stft(inputs)\n",
    "\n",
    "    # Split mixture into subbands as separate channels\n",
    "    mix = ops.reshape(x, (-1, x.shape[1], x.shape[2] // n_subbands, 2 * n_subbands))\n",
    "    first_conv_out = layers.Conv2D(channels, 1, 1, use_bias=False)(mix)\n",
    "    x = first_conv_out\n",
    "\n",
    "    # Encoder path\n",
    "    encoder_outs = []\n",
    "    for _ in range(n_scales):\n",
    "        x = TimeFrequencyTransformBlock(\n",
    "            channels, block_size, fft_dim, bottleneck_factor\n",
    "        )(x)\n",
    "        encoder_outs.append(x)\n",
    "        fft_dim, channels = fft_dim // scale[0], channels + growth\n",
    "        x = Downscale(channels, scale)(x)\n",
    "\n",
    "    # Bottleneck\n",
    "    x = TimeFrequencyTransformBlock(channels, block_size, fft_dim, bottleneck_factor)(x)\n",
    "\n",
    "    # Decoder path\n",
    "    for _ in range(n_scales):\n",
    "        fft_dim, channels = fft_dim * scale[0], channels - growth\n",
    "        x = ops.concatenate([Upscale(channels, scale)(x), encoder_outs.pop()], axis=-1)\n",
    "        x = TimeFrequencyTransformBlock(\n",
    "            channels, block_size, fft_dim, bottleneck_factor, in_channels=x.shape[-1]\n",
    "        )(x)\n",
    "\n",
    "    # Residual connection and final convolutions\n",
    "    x = ops.concatenate([mix, x * first_conv_out], axis=-1)\n",
    "    x = layers.Conv2D(channels, 1, 1, use_bias=False, activation=\"gelu\")(x)\n",
    "    x = layers.Conv2D(n_instruments * n_subbands * 2, 1, 1, use_bias=False)(x)\n",
    "\n",
    "    # Reshape back to instrument-wise STFT\n",
    "    x = ops.reshape(x, (-1, x.shape[1], x.shape[2] * n_subbands, n_instruments, 2))\n",
    "    x = ops.transpose(x, (0, 3, 1, 2, 4))\n",
    "    x = ops.reshape(x, (-1, n_instruments, x.shape[2], x.shape[3] * 2))\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=x, **kwargs)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Loss and Metrics\n",
    "\n",
    "We define:\n",
    "\n",
    "- `spectral_loss`: Mean absolute error in STFT domain.\n",
    "- `sdr`: Signal-to-Distortion Ratio, a common source separation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def prediction_to_wave(x, n_instruments=N_INSTRUMENTS):\n",
    "    \"\"\"Convert STFT predictions back to waveform.\"\"\"\n",
    "    x = ops.reshape(x, (-1, x.shape[2], x.shape[3] // 2, 2))\n",
    "    x = inverse_stft(x)\n",
    "    return ops.reshape(x, (-1, n_instruments, x.shape[1]))\n",
    "\n",
    "\n",
    "def target_to_stft(y):\n",
    "    \"\"\"Convert target waveforms to their STFT representations.\"\"\"\n",
    "    y = ops.reshape(y, (-1, CHUNK_SIZE))\n",
    "    y_real, y_imag = ops.stft(y, STFT_N_FFT, STFT_HOP_LENGTH, STFT_N_FFT)\n",
    "    y_real, y_imag = y_real[..., :-1], y_imag[..., :-1]\n",
    "    y = ops.stack([y_real, y_imag], axis=-1)\n",
    "    return ops.reshape(y, (-1, N_INSTRUMENTS, y.shape[1], y.shape[2] * 2))\n",
    "\n",
    "\n",
    "@saving.register_keras_serializable()\n",
    "def sdr(y_true, y_pred):\n",
    "    \"\"\"Signal-to-Distortion Ratio metric.\"\"\"\n",
    "    y_pred = prediction_to_wave(y_pred)\n",
    "    # Add epsilon for numerical stability\n",
    "    num = ops.sum(ops.square(y_true), axis=-1) + 1e-8\n",
    "    den = ops.sum(ops.square(y_true - y_pred), axis=-1) + 1e-8\n",
    "    return 10 * ops.log10(num / den)\n",
    "\n",
    "\n",
    "@saving.register_keras_serializable()\n",
    "def spectral_loss(y_true, y_pred):\n",
    "    \"\"\"Mean absolute error in the STFT domain.\"\"\"\n",
    "    y_true = target_to_stft(y_true)\n",
    "    return ops.mean(ops.absolute(y_true - y_pred))\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Training\n",
    "\n",
    "### Visualize Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Load or create the model\n",
    "if path.exists(MODEL_PATH):\n",
    "    model = saving.load_model(MODEL_PATH)\n",
    "else:\n",
    "    model = build_model(keras.Input(sample_batch_x.shape[1:]), name=\"tfc_tdf_net\")\n",
    "\n",
    "# Display the model architecture\n",
    "model.summary()\n",
    "img = keras.utils.plot_model(model, path.join(TMP_DIR, \"model.png\"), show_shapes=True)\n",
    "display.display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Compile and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = keras.optimizers.Adam(5e-05, gradient_accumulation_steps=ACCUMULATION_STEPS)\n",
    "model.compile(optimizer=optimizer, loss=spectral_loss, metrics=[sdr])\n",
    "\n",
    "# Define callbacks\n",
    "cbs = [\n",
    "    callbacks.ModelCheckpoint(MODEL_PATH, \"val_sdr\", save_best_only=True, mode=\"max\"),\n",
    "    callbacks.ReduceLROnPlateau(factor=0.95, patience=2),\n",
    "    callbacks.CSVLogger(CSV_LOG_PATH),\n",
    "]\n",
    "\n",
    "if not path.exists(MODEL_PATH):\n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=cbs, shuffle=False)\n",
    "else:\n",
    "    # Demonstration of a single epoch of training when model already exists\n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=1, shuffle=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluate the model on the validation dataset and visualize predicted vocals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model.evaluate(val_ds, verbose=2)\n",
    "y_pred = model.predict(sample_batch_x, verbose=2)\n",
    "y_pred = prediction_to_wave(y_pred)\n",
    "visualize_audio_np(ops.convert_to_numpy(y_pred[0, 0]), name=\"vocals_pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "We built and trained a vocal track separation model using an encoder-decoder\n",
    "architecture with custom blocks applied to the MUSDB18 dataset. We demonstrated\n",
    "STFT-based preprocessing, data augmentation, and a source separation metric (SDR).\n",
    "\n",
    "**Next steps:**\n",
    "\n",
    "- Train for more epochs and refine hyperparameters.\n",
    "- Separate multiple instruments simultaneously.\n",
    "- Enhance the model to handle instruments not present in the mixture."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "vocal_track_separation",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}