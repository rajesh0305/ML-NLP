{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Writing a training loop from scratch in PyTorch\n",
    "\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Date created:** 2023/06/25<br>\n",
    "**Last modified:** 2023/06/25<br>\n",
    "**Description:** Writing low-level training & evaluation loops in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install keras --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# This guide can only be run with the torch backend.\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import torch\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Keras provides default training and evaluation loops, `fit()` and `evaluate()`.\n",
    "Their usage is covered in the guide\n",
    "[Training & evaluation with the built-in methods](/guides/training_with_built_in_methods/).\n",
    "\n",
    "If you want to customize the learning algorithm of your model while still leveraging\n",
    "the convenience of `fit()`\n",
    "(for instance, to train a GAN using `fit()`), you can subclass the `Model` class and\n",
    "implement your own `train_step()` method, which\n",
    "is called repeatedly during `fit()`.\n",
    "\n",
    "Now, if you want very low-level control over training & evaluation, you should write\n",
    "your own training & evaluation loops from scratch. This is what this guide is about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## A first end-to-end example\n",
    "\n",
    "To write a custom training loop, we need the following ingredients:\n",
    "\n",
    "- A model to train, of course.\n",
    "- An optimizer. You could either use a `keras.optimizers` optimizer,\n",
    "or a native PyTorch optimizer from `torch.optim`.\n",
    "- A loss function. You could either use a `keras.losses` loss,\n",
    "or a native PyTorch loss from `torch.nn`.\n",
    "- A dataset. You could use any format: a `tf.data.Dataset`,\n",
    "a PyTorch `DataLoader`, a Python generator, etc.\n",
    "\n",
    "Let's line them up. We'll use torch-native objects in each case --\n",
    "except, of course, for the Keras model.\n",
    "\n",
    "First, let's get the model and the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Let's consider a simple MNIST model\n",
    "def get_model():\n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "    x1 = keras.layers.Dense(64, activation=\"relu\")(inputs)\n",
    "    x2 = keras.layers.Dense(64, activation=\"relu\")(x1)\n",
    "    outputs = keras.layers.Dense(10, name=\"predictions\")(x2)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create load up the MNIST dataset and put it in a torch DataLoader\n",
    "# Prepare the training dataset.\n",
    "batch_size = 32\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784)).astype(\"float32\")\n",
    "x_test = np.reshape(x_test, (-1, 784)).astype(\"float32\")\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "# Reserve 10,000 samples for validation.\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# Create torch Datasets\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(x_train), torch.from_numpy(y_train)\n",
    ")\n",
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.from_numpy(x_val), torch.from_numpy(y_val)\n",
    ")\n",
    "\n",
    "# Create DataLoaders for the Datasets\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Next, here's our PyTorch optimizer and our PyTorch loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Instantiate a torch optimizer\n",
    "model = get_model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Instantiate a torch loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's train our model using mini-batch gradient with a custom training loop.\n",
    "\n",
    "Calling `loss.backward()` on a loss tensor triggers backpropagation.\n",
    "Once that's done, your optimizer is magically aware of the gradients for each variable\n",
    "and can update its variables, which is done via `optimizer.step()`.\n",
    "Tensors, variables, optimizers are all interconnected to one another via hidden global state.\n",
    "Also, don't forget to call `model.zero_grad()` before `loss.backward()`, or you won't\n",
    "get the right gradients for your variables.\n",
    "\n",
    "Here's our training loop, step by step:\n",
    "\n",
    "- We open a `for` loop that iterates over epochs\n",
    "- For each epoch, we open a `for` loop that iterates over the dataset, in batches\n",
    "- For each batch, we call the model on the input data to retrive the predictions,\n",
    "then we use them to compute a loss value\n",
    "- We call `loss.backward()` to\n",
    "- Outside the scope, we retrieve the gradients of the weights\n",
    "of the model with regard to the loss\n",
    "- Finally, we use the optimizer to update the weights of the model based on the\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for step, (inputs, targets) in enumerate(train_dataloader):\n",
    "        # Forward pass\n",
    "        logits = model(inputs)\n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer variable updates\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log every 100 batches.\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"Training loss (for 1 batch) at step {step}: {loss.detach().numpy():.4f}\"\n",
    "            )\n",
    "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "As an alternative, let's look at what the loop looks like when using a Keras optimizer\n",
    "and a Keras loss function.\n",
    "\n",
    "Important differences:\n",
    "\n",
    "- You retrieve the gradients for the variables via `v.value.grad`,\n",
    "called on each trainable variable.\n",
    "- You update your variables via `optimizer.apply()`, which must be\n",
    "called in a `torch.no_grad()` scope.\n",
    "\n",
    "**Also, a big gotcha:** while all NumPy/TensorFlow/JAX/Keras APIs\n",
    "as well as Python `unittest` APIs use the argument order convention\n",
    "`fn(y_true, y_pred)` (reference values first, predicted values second),\n",
    "PyTorch actually uses `fn(y_pred, y_true)` for its losses.\n",
    "So make sure to invert the order of `logits` and `targets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nStart of epoch {epoch}\")\n",
    "    for step, (inputs, targets) in enumerate(train_dataloader):\n",
    "        # Forward pass\n",
    "        logits = model(inputs)\n",
    "        loss = loss_fn(targets, logits)\n",
    "\n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        trainable_weights = [v for v in model.trainable_weights]\n",
    "\n",
    "        # Call torch.Tensor.backward() on the loss to compute gradients\n",
    "        # for the weights.\n",
    "        loss.backward()\n",
    "        gradients = [v.value.grad for v in trainable_weights]\n",
    "\n",
    "        # Update weights\n",
    "        with torch.no_grad():\n",
    "            optimizer.apply(gradients, trainable_weights)\n",
    "\n",
    "        # Log every 100 batches.\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"Training loss (for 1 batch) at step {step}: {loss.detach().numpy():.4f}\"\n",
    "            )\n",
    "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Low-level handling of metrics\n",
    "\n",
    "Let's add metrics monitoring to this basic training loop.\n",
    "\n",
    "You can readily reuse built-in Keras metrics (or custom ones you wrote) in such training\n",
    "loops written from scratch. Here's the flow:\n",
    "\n",
    "- Instantiate the metric at the start of the loop\n",
    "- Call `metric.update_state()` after each batch\n",
    "- Call `metric.result()` when you need to display the current value of the metric\n",
    "- Call `metric.reset_state()` when you need to clear the state of the metric\n",
    "(typically at the end of an epoch)\n",
    "\n",
    "Let's use this knowledge to compute `CategoricalAccuracy` on training and\n",
    "validation data at the end of each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Get a fresh model\n",
    "model = get_model()\n",
    "\n",
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Here's our training & evaluation loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"\\nStart of epoch {epoch}\")\n",
    "    for step, (inputs, targets) in enumerate(train_dataloader):\n",
    "        # Forward pass\n",
    "        logits = model(inputs)\n",
    "        loss = loss_fn(targets, logits)\n",
    "\n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        trainable_weights = [v for v in model.trainable_weights]\n",
    "\n",
    "        # Call torch.Tensor.backward() on the loss to compute gradients\n",
    "        # for the weights.\n",
    "        loss.backward()\n",
    "        gradients = [v.value.grad for v in trainable_weights]\n",
    "\n",
    "        # Update weights\n",
    "        with torch.no_grad():\n",
    "            optimizer.apply(gradients, trainable_weights)\n",
    "\n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(targets, logits)\n",
    "\n",
    "        # Log every 100 batches.\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"Training loss (for 1 batch) at step {step}: {loss.detach().numpy():.4f}\"\n",
    "            )\n",
    "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(f\"Training acc over epoch: {float(train_acc):.4f}\")\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_state()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataloader:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_state()\n",
    "    print(f\"Validation acc: {float(val_acc):.4f}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Low-level handling of losses tracked by the model\n",
    "\n",
    "Layers & models recursively track any losses created during the forward pass\n",
    "by layers that call `self.add_loss(value)`. The resulting list of scalar loss\n",
    "values are available via the property `model.losses`\n",
    "at the end of the forward pass.\n",
    "\n",
    "If you want to be using these loss components, you should sum them\n",
    "and add them to the main loss in your training step.\n",
    "\n",
    "Consider this layer, that creates an activity regularization loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ActivityRegularizationLayer(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(1e-2 * torch.sum(inputs))\n",
    "        return inputs\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Let's build a really simple model that uses it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = keras.layers.Dense(64, activation=\"relu\")(inputs)\n",
    "# Insert activity regularization as a layer\n",
    "x = ActivityRegularizationLayer()(x)\n",
    "x = keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = keras.layers.Dense(10, name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Here's what our training loop should look like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Get a fresh model\n",
    "model = get_model()\n",
    "\n",
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nStart of epoch {epoch}\")\n",
    "    for step, (inputs, targets) in enumerate(train_dataloader):\n",
    "        # Forward pass\n",
    "        logits = model(inputs)\n",
    "        loss = loss_fn(targets, logits)\n",
    "        if model.losses:\n",
    "            loss = loss + torch.sum(*model.losses)\n",
    "\n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        trainable_weights = [v for v in model.trainable_weights]\n",
    "\n",
    "        # Call torch.Tensor.backward() on the loss to compute gradients\n",
    "        # for the weights.\n",
    "        loss.backward()\n",
    "        gradients = [v.value.grad for v in trainable_weights]\n",
    "\n",
    "        # Update weights\n",
    "        with torch.no_grad():\n",
    "            optimizer.apply(gradients, trainable_weights)\n",
    "\n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(targets, logits)\n",
    "\n",
    "        # Log every 100 batches.\n",
    "        if step % 100 == 0:\n",
    "            print(\n",
    "                f\"Training loss (for 1 batch) at step {step}: {loss.detach().numpy():.4f}\"\n",
    "            )\n",
    "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(f\"Training acc over epoch: {float(train_acc):.4f}\")\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_state()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataloader:\n",
    "        val_logits = model(x_batch_val, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_state()\n",
    "    print(f\"Validation acc: {float(val_acc):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "That's it!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "None",
  "colab": {
   "collapsed_sections": [],
   "name": "writing_a_custom_training_loop_in_torch",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}