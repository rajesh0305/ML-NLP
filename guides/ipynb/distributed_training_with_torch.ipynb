{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Multi-GPU distributed training with PyTorch\n",
    "\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Date created:** 2023/06/29<br>\n",
    "**Last modified:** 2023/06/29<br>\n",
    "**Description:** Guide to multi-GPU training for Keras models with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "There are generally two ways to distribute computation across multiple devices:\n",
    "\n",
    "**Data parallelism**, where a single model gets replicated on multiple devices or\n",
    "multiple machines. Each of them processes different batches of data, then they merge\n",
    "their results. There exist many variants of this setup, that differ in how the different\n",
    "model replicas merge results, in whether they stay in sync at every batch or whether they\n",
    "are more loosely coupled, etc.\n",
    "\n",
    "**Model parallelism**, where different parts of a single model run on different devices,\n",
    "processing a single batch of data together. This works best with models that have a\n",
    "naturally-parallel architecture, such as models that feature multiple branches.\n",
    "\n",
    "This guide focuses on data parallelism, in particular **synchronous data parallelism**,\n",
    "where the different replicas of the model stay in sync after each batch they process.\n",
    "Synchronicity keeps the model convergence behavior identical to what you would see for\n",
    "single-device training.\n",
    "\n",
    "Specifically, this guide teaches you how to use PyTorch's `DistributedDataParallel`\n",
    "module wrapper to train Keras, with minimal changes to your code,\n",
    "on multiple GPUs (typically 2 to 16) installed on a single machine (single host,\n",
    "multi-device training). This is the most common setup for researchers and small-scale\n",
    "industry workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install keras --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by defining the function that creates the model that we will train,\n",
    "and the function that creates the dataset we will train on (MNIST in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    # Make a simple convnet with batch normalization and dropout.\n",
    "    inputs = keras.Input(shape=(28, 28, 1))\n",
    "    x = keras.layers.Rescaling(1.0 / 255.0)(inputs)\n",
    "    x = keras.layers.Conv2D(filters=12, kernel_size=3, padding=\"same\", use_bias=False)(\n",
    "        x\n",
    "    )\n",
    "    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n",
    "    x = keras.layers.ReLU()(x)\n",
    "    x = keras.layers.Conv2D(\n",
    "        filters=24,\n",
    "        kernel_size=6,\n",
    "        use_bias=False,\n",
    "        strides=2,\n",
    "    )(x)\n",
    "    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n",
    "    x = keras.layers.ReLU()(x)\n",
    "    x = keras.layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=6,\n",
    "        padding=\"same\",\n",
    "        strides=2,\n",
    "        name=\"large_k\",\n",
    "    )(x)\n",
    "    x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n",
    "    x = keras.layers.ReLU()(x)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    outputs = keras.layers.Dense(10)(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    # Load the data and split it between train and test sets\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Scale images to the [0, 1] range\n",
    "    x_train = x_train.astype(\"float32\")\n",
    "    x_test = x_test.astype(\"float32\")\n",
    "    # Make sure images have shape (28, 28, 1)\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "    print(\"x_train shape:\", x_train.shape)\n",
    "\n",
    "    # Create a TensorDataset\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(x_train), torch.from_numpy(y_train)\n",
    "    )\n",
    "    return dataset\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Next, let's define a simple PyTorch training loop that targets\n",
    "a GPU (note the calls to `.cuda()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, dataloader, num_epochs, optimizer, loss_fn):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_loss_count = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs = inputs.cuda(non_blocking=True)\n",
    "            targets = targets.cuda(non_blocking=True)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_loss_count += 1\n",
    "\n",
    "        # Print loss statistics\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "            f\"Loss: {running_loss / running_loss_count}\"\n",
    "        )\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Single-host, multi-device synchronous training\n",
    "\n",
    "In this setup, you have one machine with several GPUs on it (typically 2 to 16). Each\n",
    "device will run a copy of your model (called a **replica**). For simplicity, in what\n",
    "follows, we'll assume we're dealing with 8 GPUs, at no loss of generality.\n",
    "\n",
    "**How it works**\n",
    "\n",
    "At each step of training:\n",
    "\n",
    "- The current batch of data (called **global batch**) is split into 8 different\n",
    "sub-batches (called **local batches**). For instance, if the global batch has 512\n",
    "samples, each of the 8 local batches will have 64 samples.\n",
    "- Each of the 8 replicas independently processes a local batch: they run a forward pass,\n",
    "then a backward pass, outputting the gradient of the weights with respect to the loss of\n",
    "the model on the local batch.\n",
    "- The weight updates originating from local gradients are efficiently merged across the 8\n",
    "replicas. Because this is done at the end of every step, the replicas always stay in\n",
    "sync.\n",
    "\n",
    "In practice, the process of synchronously updating the weights of the model replicas is\n",
    "handled at the level of each individual weight variable. This is done through a **mirrored\n",
    "variable** object.\n",
    "\n",
    "**How to use it**\n",
    "\n",
    "To do single-host, multi-device synchronous training with a Keras model, you would use\n",
    "the `torch.nn.parallel.DistributedDataParallel` module wrapper.\n",
    "Here's how it works:\n",
    "\n",
    "- We use `torch.multiprocessing.start_processes` to start multiple Python processes, one\n",
    "per device. Each process will run the `per_device_launch_fn` function.\n",
    "- The `per_device_launch_fn` function does the following:\n",
    "    - It uses `torch.distributed.init_process_group` and `torch.cuda.set_device`\n",
    "    to configure the device to be used for that process.\n",
    "    - It uses `torch.utils.data.distributed.DistributedSampler`\n",
    "    and `torch.utils.data.DataLoader` to turn our data into a distributed data loader.\n",
    "    - It also uses `torch.nn.parallel.DistributedDataParallel` to turn our model into\n",
    "    a distributed PyTorch module.\n",
    "    - It then calls the `train_model` function.\n",
    "- The `train_model` function will then run in each process, with the model using\n",
    "a separate device in each process.\n",
    "\n",
    "Here's the flow, where each step is split into its own utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "num_gpu = torch.cuda.device_count()\n",
    "num_epochs = 2\n",
    "batch_size = 64\n",
    "print(f\"Running on {num_gpu} GPUs\")\n",
    "\n",
    "\n",
    "def setup_device(current_gpu_index, num_gpus):\n",
    "    # Device setup\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"56492\"\n",
    "    device = torch.device(\"cuda:{}\".format(current_gpu_index))\n",
    "    torch.distributed.init_process_group(\n",
    "        backend=\"nccl\",\n",
    "        init_method=\"env://\",\n",
    "        world_size=num_gpus,\n",
    "        rank=current_gpu_index,\n",
    "    )\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    torch.distributed.destroy_process_group()\n",
    "\n",
    "\n",
    "def prepare_dataloader(dataset, current_gpu_index, num_gpus, batch_size):\n",
    "    sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        dataset,\n",
    "        num_replicas=num_gpus,\n",
    "        rank=current_gpu_index,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        sampler=sampler,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def per_device_launch_fn(current_gpu_index, num_gpu):\n",
    "    # Setup the process groups\n",
    "    setup_device(current_gpu_index, num_gpu)\n",
    "\n",
    "    dataset = get_dataset()\n",
    "    model = get_model()\n",
    "\n",
    "    # prepare the dataloader\n",
    "    dataloader = prepare_dataloader(dataset, current_gpu_index, num_gpu, batch_size)\n",
    "\n",
    "    # Instantiate the torch optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Instantiate the torch loss function\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Put model on device\n",
    "    model = model.to(current_gpu_index)\n",
    "    ddp_model = torch.nn.parallel.DistributedDataParallel(\n",
    "        model, device_ids=[current_gpu_index], output_device=current_gpu_index\n",
    "    )\n",
    "\n",
    "    train_model(ddp_model, dataloader, num_epochs, optimizer, loss_fn)\n",
    "\n",
    "    cleanup()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Time to start multiple processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # We use the \"fork\" method rather than \"spawn\" to support notebooks\n",
    "    torch.multiprocessing.start_processes(\n",
    "        per_device_launch_fn,\n",
    "        args=(num_gpu,),\n",
    "        nprocs=num_gpu,\n",
    "        join=True,\n",
    "        start_method=\"fork\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "That's it!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "distributed_training_with_torch",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}